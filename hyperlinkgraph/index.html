<!DOCTYPE html>
<html><head><title>WDC - Hyperlink Graph</title>
<link rel="stylesheet" href="../style.css" type="text/css" media="screen"/>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="author" content="Robert Meusel, Christian Bizer, Oliver Lehmberg">
<meta name="keywords" content="Hyperlink Graph, WebGraph, Web Graph">
<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script type="text/javascript" src="/jquery.toc.min.js"></script>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-30248817-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

</head>
<body> 

  <div id="logo" style="text-align:right; background-color: white;">&nbsp;&nbsp;<a href="http://dws.informatik.uni-mannheim.de"><img src="../images/ma-logo.gif" alt="University of Mannheim - Logo"></a>&nbsp;&nbsp;<br></div>


<div id="header">
<h1 style="font-size: 250%;">Web Data Commons - Hyperlink Graph</h1>
</div>

<div id="tagline">Extracting the Hyperlink Graph from the Common Web Crawl</div>

<div id="authors">
<a href="http://dws.informatik.uni-mannheim.de/en/people/researchers/robert-meusel/">Robert Meusel</a><br />
<a href="http://dws.informatik.uni-mannheim.de/en/people/researchers/oliver-lehmberg/">Oliver Lehmberg</a><br />
<a href="http://dws.informatik.uni-mannheim.de/en/people/professors/prof-dr-christian-bizer/">Christian Bizer</a><br />
<a href="http://vigna.di.unimi.it/">Sebastiano Vigna</a><br />
<br/>
<br/>
</div>

<div id="content">

<p>
This page provides an overview about currently two large hyperlink graph for public download. The graphs has been extracted from the <a href="http://commoncrawl.org/">Common Crawl</a> 2012 and 2014 web corpus. The 2012 graph covers <b>3.5 billion web pages and 128 billion hyperlinks</b> between these pages. To the best of our knowledge, the graph is the largest hyperlink graph that is available to the public outside companies such as Google, Yahoo, and Microsoft. The later graph from 2014 consists of <b>1.7 billion web pages connected by 64 billion hyperlinks</b>.

Below we provide instructions on how to download the graph as well as basic statistics about its topology.
</p>
<p>We hope that the graph will be useful for researchers who develop
<ul>
<li><b>search algorithms</b> that rank results based on the hyperlinks between pages.</li>
<li><b>SPAM detection methods</b> which identity networks of web pages that are published in order to trick search engines.</li>
<li><b>graph analysis algorithms</b> and can use the hyperlink graph for testing the scalability and performance of their tools.</li>
<li><b>Web Science researchers</b> who want to analyze the linking patterns within specific topical domains in order to identify the social mechanisms that govern these domains.</li>
</ul>
</p>


<h2>Contents</h2>
<div id="toc" class="toc"></div>
<div id="toccontent">

<h2 id="aggregationlevels">1. Levels of Aggregation</h2>
<p>
We provide the hyperlink graphs on four different levels of aggregation:  
<ul>
<li><strong>Page-Level Graph</strong> - This version of the graph contains all details with each node representing a single web page (like <code>http://dws.informatik.uni-mannheim.de/en/projects/current-projects/#c13686</code>) and each arc a hyperlink between to two pages.</li>
<li><strong>Host Graph</strong> - This graph aggregates the page graph by subdomain/host. Each node in the graph represents a specific subdomain/host (like <code>research.dws.uni-mannheim.de</code>) and a arc exists, if at least one hyperlink was found between pages that belong to a pair of subdomains/hosts. Note that subdomains/hosts can be of arbitrary depth.</li>
<li><strong>Pay-Level-Domain Graph</strong> - Each node represents a pay-level-domain (like <code>uni-mannheim.de</code>). An arc exists if at least one hyperlink was found between pages contained in a pair pay-level-domains.</li>
</ul>
<h2 id="availablegraphs">2. Available Web Graphs</h2>
<p>
Till today, we have extracted the graph structured from two available Common Crawl Web Crawls. The first was extracted from the crawl gathered in the first half of 2012, released in August 2012. The second, latest graph was extracted from the crawl gathered in the first quarter of 2014, released in April 2014.
As described above, we provide three different levels of granularities for each graph.
</p>
<h2 id="graph2012">2.2. Extracted Hyperlink Graph from August 2012 Common Crawl Corpus</h2>
<p>
The dataset released in August 2012 by the Common Crawl Foundation, was gathered using a web crawler making use of a breadth-first-search selection strategy. The crawl was seeded with a large number of URLs from former crawls of the foundation. 
</br>
The table below gives an overview of the size of the different aggregation levels of the extracted graph which are available as download:
</p>
<table>
<tr><th>Granularity</th><th>#Nodes</th><th>#Arcs</th></tr>
<tr><td>Page </td><td align="right">3,563 million</td><td align="right">128,736 million</td></tr>
<tr><td>Host </td><td align="right">101 million</td><td align="right">2,043 million</td></tr>
<tr><td>Pay-Level-Domain </td><td align="right">43 million</td><td align="right">623 million</td></tr>
</table>
<p>
Within the extracted graph over 94% of all crawled pages are weakly connected and over 50% are in addition strongly connected. This makes this graph useful for a wide range of graph analysis algorithms and structural analysis.
Please visit the following links for more detailed statistics about the graph as well as detailed statistics how to download and make use of the graph:

<ul>
<li><a href="2012-08/topology.html">Basic statistics about the topology of the graphs</a></li>
<li><a href="2012-08/download.html">Download Instructions</a></li>
</ul>
</p>
<h2 id="graph2014">2.2. Extracted Hyperlink Graph from Spring 2014 Common Crawl Corpus</h2>
<p>
The dataset released in April 2014 by the Common Crawl Foundation, was gathered using a modification of the <a href="http://nutch.apache.org/">Apache Nutch</a> crawler. The crawler was modified to not follow links, but to simply crawl all given seeds. The crawl was seeded with over 6 billion URLs provided by the search engine company <a href="http://blekko.com/">blekko</a> and the crawler was stopped at around 2 billion gathered pages. The Common Crawl Foundation and blekko started a cooperation to increase the quality and popularity of the crawled pages and reduce the number of crawled spam pages and crawler traps.
<br/>
The table below gives an overview of the size of the different aggregation levels of the graph which are available as download:
</p>
<table>
<tr><th>Granularity</th><th>#Nodes</th><th>#Arcs</th></tr>
<tr><td>Page </td><td align="right">1,727 million</td><td align="right">64,422 million</td></tr>
<tr><td>Host </td><td align="right">22 million</td><td align="right">123 million</td></tr>
<tr><td>Pay-Level-Domain </td><td align="right">13 million</td><td align="right">56 million</td></tr>
</table>
<p>
Within the extracted graph over 91% of all crawled pages are weakly connected, but only around 19% of the crawled pages are strongly connected. This is due to new crawling strategy and the incomplete gathering of the seed URLs. 
<br/>
<font color="red">Due to crawling strategy and the resulting sparse connected graph of the 2014 web crawl, we recommend to use the 2012 graph dataset when analysing structural behaviours of the WWW.</font>
<br/>
Please visit the following links for more detailed statistics about the 2014 graph as well as detailed statistics how to download and make use of the graph:

<ul>
<li><a href="2014-04/topology.html">Basic statistics about the topology of the graphs</a></li>
<li><a href="2014-04/download.html">Download Instructions</a></li>
</ul>
</p>
<h2 id="examples">3. Data Formats and Download</h2>

<p>
We provide the graphs for free download in several formats. All graphs are provided in an index/arc data format. In addition, we provide the page graph in the format used by the <a href="http://webgraph.di.unimi.it/" target="_blank">WebGraph library</a> and the PLD graph in the format used by <a href="http://pajek.imfm.si/doku.php" target="_blank">Pajek</a>. The page graphs are hosted on Amazon S3. The aggregated graphs are provided for download via a server in Mannheim, Germany.
</p>


<h3 id="about">3.1. Index/Arc Format</h3>

<p>
The Index/Arc format represents each graph using two files. Within the index file each line represents one node. The first column states the node name, the second column states the node index. Within the arc file each line represents a directed edge between two nodes, where the first column is the origin node and the second the target node. The files are sorted by index and use tabs as a delimiter.

The following example files contain a graph with 106 nodes and 141 arcs. 
<p>

<ul>
<li><a href="data/example_index">example index  file (106 nodes)</a></li>
<li><a href="data/example_arcs">example arc file (141 arcs)</a></li>
</ul>

</p><p>The following table contains the links for downloading the graphs.<p>
<table>
<tr><th>Data Set</th><th>2012</th><th>2014 </th></tr>
<tr><td>Page Graph</td><td>see below (45 + 331 GB)</td><td>see below (45 + 331 GB)</td></tr>
<tr><td>Host Graph</td><td><a href="http://data.dws.informatik.uni-mannheim.de/hyperlinkgraph/2012-08/sd-index.gz">Index</a> (832 MB) | <a href="http://data.dws.informatik.uni-mannheim.de/hyperlinkgraph/2012-08/sd-arc.gz">Arcs</a> (9.2 GB) </td><td></td></tr>
<tr><td>PLD Graph</td><td><a href="http://data.dws.informatik.uni-mannheim.de/hyperlinkgraph/2012-08/pld-index.gz">Index</a> (297 MB) | <a href="http://data.dws.informatik.uni-mannheim.de/hyperlinkgraph/2012-08/pld-arc.gz">Arcs</a> (2.8 GB)</td><td></td></tr>
</table>
In case you experience any problems downloading the files please find additional information <a href="https://groups.google.com/forum/?fromgroups=#!topic/web-data-commons/SW7Gw2cLDr8" target="_blank">here</a>.
<br><br>
<b>Downloading the page graph:</b> The page graph (arc and indes files) are, due to their size split into in small files of around 500 MB. These files can be downloaded using <code>wget -i http://webdatacommons.org/hyperlinkgraph/data/index.list.txt</code> for the index files and respectively <code>wget -i http://webdatacommons.org/hyperlinkgraph/data/arc.list.txt</code> for the arc files.

</p>
<h3 id="about">3.2. WebGraph Framework Format</h3>
<p>
We also provide the page graph in the format expected by the <a href="http://webgraph.di.unimi.it/" target="_blank">WebGraph Framework</a> developed by Sebastiano Vigna. The graph is represented using three files: <code>.graph, .offsets, .properties</code>. All three are necessary to load the network into the library.
<br/>
Using the WebGraph Framework, which can be downloaded from <a href="http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22it.unimi.dsi%22">Maven Central</a>, these files can be loaded using the following line of code: 
<code>BVGraph graph = BVGraph.loadMapped(baseName, new ProgressLogger())</code>.
</p>
<h3 id="about">3.3. Pajek NET Format</h3>
<p>
We offer the PLD-aggregation of the page graph also in the <a href="https://gephi.org/users/supported-graph-formats/pajek-net-format/" target="_blank">Pajek NET Format</a> which is understood by various graph analysis tools such as <a href="http://pajek.imfm.si/doku.php" target="_blank">Pajek</a> or <a href="https://gephi.org/" target="_blank">Gephi</a>. The format combines the index and the arc list into a single file (<a href="data/example.net">example file</a>, 106 nodes, 141 arcs).
</p>

<h2 id="process">4. Extraction Process and Source Code</h2>

<p>The WDC Hyperlink Graph was extracted from the latest version of the <a href="http://commoncrawl.org/">Common Crawl </a>, which was gathered in the first half of 2012. From this corpus, we extracted all HTML pages (mime-type: <i>text/html</i>) and every hyperlink pointing to another crawled HTML page (link type: <i>a</i> and <i>link</i>). For each re-direct, we include an additional node in the graph which links to the re-direct target. 
</p>
<p>
Since the Common Crawl coprus is provided via the <a href="http://aws.amazon.com/s3/">AWS Simple Storage Service (S3)</a>, it made sense to perform the extraction in the <a href="http://aws.amazon.com/ec2/">Amazon cloud (EC2)</a>. The main criterion here is the cost to achieve a certain task. Instead of using the ubiquitous <a href="http://hadoop.apache.org/">Hadoop framework</a>, we found using the <a href="http://aws.amazon.com/sqs/">Simple Queue Service (SQS)</a> for our extraction process increased efficiency. SQS provides a message queue implementation, which we use to coordinate the extraction nodes. The Common Crawl data set is readily partitioned into compressed files of around 100MB each including several thousand webpages. Beside those content files, also <a href="https://commoncrawl.atlassian.net/wiki/display/CRWL/About+the+Data+Set">metadata files</a> are provided. These files include for each page the URL, re-directs, mime-type, hyperlinks, and type of link using a JSON format. As these files contain all information needed to extract the hyperlink graph for the crawled webpages, we used an adapted version of the framework that we already used to extract <a href="../2012-08/stats/stats.html">RDFa, Microformat and Microdata</a> from the crawled pages to parse the URL, redirect, links and link types from the metadata files. We used 100 machines on Amazon EC2 to process the metadata files.
In a second step, we created an index file for each aggregation level (PLD, Domain, 1st Subdomain) and indexed the graphs based on this mappings using <a href="http://pig.apache.org/">Apache PIG</a> running on a 40 node <a href="http://pig.apache.org/">Amazon Elastic MapReduce cluster (EMR)</a>.
</p>
<p>
The source code to extract the WDC Hyperlink Graph from the Common Crawl corpus can be checked out from our <a href="https://www.assembla.com/code/commondata/subversion/nodes/247/webgraph/extractor" target="_blank">Subversion repository</a>. For using the code, you will need to create your own configuration and fill in your AWS authentication information and bucket names. Compilation is performed using <a href="http://maven.apache.org/">Maven</a>, thus changing into the source root directory and typing <code>mvn install</code> should be sufficient to create a build. In order to run the extractor on more than 10 EC2 nodes, you will have to request an <a href="http://aws.amazon.com/contact-us/ec2-request/">EC2 instance limit increase</a> for your AWS account. Beside the raw extraction framework, the project also includes various algorithms to format and manipulate the entire graph, as shrink them to a specific aggregation level or index the graph to compress it.
</p><p>
 
</p>

<h2 id="related">5. Other Public Hyperlink Graphs and Web Crawls</h2>
<p>The <a href="http://law.di.unimi.it/datasets.php">Laboratory for Web Algorithms</a> provides various hyperlink graphs for public download in the format understood by the WebGraph Framework. In comparison to these graphs, the WDC Hyperlink Graph is more recent and larger.<br>
The <a href="http://snap.stanford.edu/data/#web">Stanford Large Network Dataset Collection</a> also contains several smaller hyperlink graphs (all below 1 million nodes).</p>
<p>Beside of the Common Crawl that was used to extract the WDC Hyperlink Graph, there are several other public web corpora that could be used to extract hyperlink graphs:</p>
<ul>
<li>The <a href="http://lemurproject.org/clueweb12/">ClueWeb12 corpus</a> was crawled in a similar time period as the Common Crawl. The corpus consists of 740 million English webpages. In comparison, the Common Crawl is 4 times larger and covers non-English top-level domains as well.</li>
<li>
The <a href="http://dbpubs.stanford.edu:8091/~testbed/doc2/WebBase/">Stanford WebBase project</a> provides a Web crawl containing 118 million pages and around 1 billion links. The corpus was collected in 2001.
</li>
<li>
The <a href="http://webscope.sandbox.yahoo.com/catalog.php?datatype=g">Yahoo! Webscope Project</a> has published an older version of the AltaVista crawl, created in 2002. The corpus includes 1.4 billion webpages that are connected by 6.6 billion hyperlinks.
</li>
</ul>
<h2 id="license">6. License</h2>
<p>The extracted data is provided according the same <a href="http://commoncrawl.org/about/terms-of-use/full-terms-of-use/">terms of use, disclaimer of warranties and limitation of liabilities</a> that apply to the Common Crawl corpus.</p>

<p>The Web Data Commons extraction framework can be used under the terms of the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache Software License</a>. </p>

<h2 id="feedback">7. Feedback</h2>
<p>Please send questions and feedback to the <a href="http://groups.google.com/group/web-data-commons">Web Data Commons mailing list</a> or post them in our <a href="https://groups.google.com/forum/?fromgroups#!forum/web-data-commons">Web Data Commons Google Group</a>.<br/><br/>
More information about Web Data Commons is found <a href="../index.html">here</a>.</p>

<h2 id="acknowledgments">8. Credits</h2>

<p>Lots of thanks to</p>
<ul>
  	<li>the <a href="http://commoncrawl.org/">Common Crawl project</a> for providing their great web crawl and thus enabling the creation of the WDC Hyperlink Graph.</li>
	<li>Sebastiano Vigna for providing and supporting us with his amazing Java <a href="http://webgraph.di.unimi.it/">WebGraph</a> library.</li>
	<li>Andrej Mrvar for his fast and detailed answers about the usage of specific functions in <a href="http://pajek.imfm.si/doku.php">Pajek</a>.</li>
	<li><a href="http://www.mpi-inf.mpg.de/~sseufert/">Stephan Seufert</a> for giving us some initial ideas about how to compress and format our graph.</li>
</ul>
<p>The creation of the WDC Hyperlink Graph was supported by the EU research project <a href="http://planet-data.eu">PlanetData</a> and by <a href="http://aws.amazon.com/education/">Amazon Web Services in Education Grant award</a>. We thank your sponsors a lot for supporting <a href="../index.html">Web Data Commons</a>. 
</p>

<a href="http://planet-data.eu"><img src="../images/pd.gif" alt="PlanetData Logo"></a>&nbsp;&nbsp;&nbsp;
<a href="http://aws.amazon.com/education/"><img src="../images/aws.png" alt="AWS Logo"></a>&nbsp;&nbsp;&nbsp;
<h2 id="references">9. References</h2>

<ul>
	<li>Oliver Lehmberg, Robert Meusel, Christian Bizer: <b><a href="http://dws.informatik.uni-mannheim.de/fileadmin/lehrstuehle/ki/pub/Lehmberg-etal-GraphStructureOfTheWebPLD.pdf" target="_blank">The Graph Structure of the Web aggregated by Pay-Level Domain</a></b>. Accepted paper at the <a href="http://www.websci14.org/">ACM Web Science 2014 Conference</a> (WebSci2014), Bloomington, USA, June 2014.</li>
	<li>Robert Meusel, Sebastiano Vigna, Oliver Lehmberg, Christian Bizer: <b><a href="https://www.wim.uni-mannheim.de/fileadmin/lehrstuehle/ki/pub/Meusel-etal-GraphStructureOfTheWeb.pdf">Graph Structure in the Web - Revisited</a></b>. Accepted paper at the <a href="http://www2014.kr/">23rd International World Wide Web Conference</a> (WWW2014), Web Science Track, Seoul, Korea, April 2014.</li>
</ul>

</div>

</div>

<script type="text/javascript">
$('#toc').toc({
    'selectors': 'h2,h3', //elements to use as headings
    'container': '#toccontent', //element to find all selectors in
    'smoothScrolling': true, //enable or disable smooth scrolling on click
    'prefix': 'toc', //prefix for anchor tags and class names
    'highlightOnScroll': true, //add class to heading that is currently in focus
    'highlightOffset': 100, //offset to trigger the next headline
    'anchorName': function(i, heading, prefix) { //custom function for anchor name
        return prefix+i;
    } 
});
</script>

</body>
</html> 
