<!DOCTYPE html>
<html><head><title>WDC - Extraction Framework</title>
<link rel="stylesheet" href="../style.css" type="text/css" media="screen"/>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="author" content="Robert Meusel, Hannes Mühleisen, Christian Bizer, Oliver Lehmberg">
<meta name="keywords" content="Distributed Framework, Java, Scalable, Cloud">
<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script type="text/javascript" src="/jquery.toc.min.js"></script>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-30248817-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
<style>
td.java, td.java-ln {vertical-align:top;}
tt.java, tt.java-ln, pre.java, pre.java-ln {line-height:1em; margin-bottom:0em;}
td.java-ln { text-align:right; }
tt.java-ln, pre.java-ln { color:#888888 }
/* Background       */ span.java0  { font-size: 10pt; color:#ffffff; }
/* Line numbers       */ span.java1  { font-size: 10pt; color:#808080; }
/* Multi-line comments       */ span.java2  { font-size: 10pt; color:#3f7f5f; }
/* Single-line comments       */ span.java3  { font-size: 10pt; color:#3f7f5f; }
/* Keywords       */ span.java4  { font-size: 10pt; color:#7f0055; font-weight:bold; }
/* Strings       */ span.java5  { font-size: 10pt; color:#2a00ff; }
/* Character constants       */ span.java6  { font-size: 10pt; color:#990000; }
/* Numeric constants       */ span.java7  { font-size: 10pt; color:#990000; }
/* Parenthesis       */ span.java8  { font-size: 10pt; color:#000000; }
/* Primitive Types       */ span.java9  { font-size: 10pt; color:#7f0055; font-weight:bold; }
/* Others       */ span.java10  { font-size: 10pt; color:#000000; }
/* Javadoc keywords       */ span.java11  { font-size: 10pt; color:#7f9fbf; }
/* Javadoc HTML tags       */ span.java12  { font-size: 10pt; color:#7f7f9f; }
/* Javadoc links       */ span.java13  { font-size: 10pt; color:#3f3fbf; }
/* Javadoc others       */ span.java14  { font-size: 10pt; color:#3f5fbf; }
/* Undefined       */ span.java15  { font-size: 10pt; color:#ff6100; }
/* Annotation       */ span.java16  { font-size: 10pt; color:#646464; }
</style>
</head>
<body> 

  <div id="logo" style="text-align:right; background-color: white;">&nbsp;&nbsp;<a href="http://dws.informatik.uni-mannheim.de"><img src="../images/ma-logo.gif" alt="University of Mannheim - Logo"></a>&nbsp;&nbsp;<br></div>


<div id="header">
<h1 style="font-size: 250%;">Web Data Commons - Framework</h1>
</div>

<div id="tagline">Distributed, Parallel Extraction from Web Crawls</div>

<div id="authors">
<a href="http://dws.informatik.uni-mannheim.de/en/people/researchers/robert-meusel/">Robert Meusel</a><br />
<a href="http://hannes.muehleisen.org/">Hannes Mühleisen</a><br />
<a href="http://dws.informatik.uni-mannheim.de/en/people/researchers/oliver-lehmberg/">Oliver Lehmberg</a><br />
Petar Petrovski<br />
<br/>
<br/>
</div>

<div id="content">

<p>
This page provides an overview about the framework which is used by the Web Data Commons project to extract datasets from the crawls provided by the <a href="http://commoncrawl.org/">Common Crawl Foundation</a>. The framework was first designed by Hannes Mühleisen for the <a href="../structureddata">extraction of Microdata, Microformats and RDFa</a> from the Common Crawl. Later extended to be able to extract the <a href="">hyperlink graph</a> as well as the <a href="">web tables</a>.

<h2>Contents</h2>
<div id="toc" class="toc"></div>
<div id="toccontent">

<h2 id="general">1. General Information</h2>
<p>
The framework was developed by the WDC to process a large number of files from the Common Crawl in parallel using the cloud services of Amazon. The software is written in Java using Maven as build tool and can be run on any operating system. 
<br>
The picture below explains the principle process flow of the framework, which is basically steered by one master node, which can be either a local server or machine, or a cloud instance itself. 
<br>
<img src="../images/framework.png" alt="WDC Framework Process Flow">
<br>
<ol>
<li>From the master node the AWS Simple Queue Service is filled with all files which should be processed. Those files represent the task for the later working instances and are basically file references.</li>
<li>From the master node a number of instances are launched within the EC2 environment of AWS. After the start up, each instances will install automatically the WDC framework and launch it.</li>
<li>Each instance, after starting the framework will automatically requests a task from the SQS and start processing the file.</li>
<li>The file will first be downloaded from S3 and will than be processed by the worker.</li>
<li>After finishing one file the result will be stored in the users own S3 Bucket. And the worker will start again at 3.</li>
<li>After the queue is empty the master can start collecting the extracted data and statistics.</li>
</ol>
</p>

<h2 id="start">2. Getting Started</h2>
<p>
Running your own extraction using the WDC Extraction Framework is rather simple and needs just some minutes to set yourself up. Please follow the steps below and make sure you have all requirements in place.
</p>

<h3 id="req">2.1. Requirements</h2>
<p>
In order to run an extraction using the WDC Framework the following is required:
<ul>
<li><b>Amazon Web Service Account/User</b>: As the framework is currently tailored to be run using services from Amazon, you will need to have an AWS with sufficient amount of credits. The user needs rights to access the AWS Services: EC2, S3, SQS and Simple DB.</li>
<li><b>AWS Access Token</b>: In order to make use of the Amazon services through the framework you need have your AWS <code>Access Key Id</code> as well as your <code>Secret Access Key</code>. Both is available through the Web Interface of AWS in the section <a href="http://aws.amazon.com/iam/">IAM</a> (See <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/IAM_Introduction.html">AWS IAM Docs</a>). In case you do not have access to this section with your user, contact the administrator of your AWS account.</li>
<li><b>Java</b>: On machine has to serve as master node. This machine, no matter if its your local server/computer or an EC2 instance within AWS needs to have Java 6 or higher installed to run the commands and control the extraction, e.g.: <pre><code>apt-get install openjdk-7-jdk</code></pre></li>
<li><b>Maven</b>: In order to compile your own version of the framework you need to have <a href="http://maven.apache.org/">Maven</a>3 in place. Building the project with Maven2 will not work, as the project makes use of certain plugins, which are unknown to version 2 of Maven: <pre><code>apt-get install maven</code></pre></li>
<li><b>Subversion</b>: In order to get the code, you need subversion installed: <pre><code>apt-get install subversion</code></pre></li>
</ul>
</p>

<h3 id="build">2.2. Building the code</h3>
<p>
Below you find a very detailed step by step description how to build the WDC Extraction Framework:
<ol>
<li>Create a new folder for the repository and navigate to the folder: <pre><code>mkdir ~/framework<br>cd ~/framework</code></pre></li>
<li>Download the code of the framework from the <a href="https://www.assembla.com/">Assembla.com</a> <a href="https://www.assembla.com/code/commondata/subversion/nodes">WDC Repository</a>. The WDC Extraction Framework code is located under <code>WDCFramework/trunk/</code>: <br><pre><code>svn checkout https://subversion.assembla.com/svn/commondata/WDCFramework/trunk</code></pre></li>
<li>Create a copy of the <code>dpef.properties.dist</code> file within the <code>/src/main/resources</code> directory and name it <code>dpef.properties</code>. Within this file all needed properties/configurations are stored: <pre><code>cp extractor/src/main/resource/dpef.properties.dist extractor/src/main/resource/dpef.properties</code></pre>
</li>
<li>Go through the file carefully and adjust at least all properties marked with <code>TODO</code>. Each property is described in more detail within the file.
The most important properties are listed below:
<ul>
<li><code>awsAccessKey</code> and <code>awsSecretKey</code>: Those keys are mandatory to get access to the AWS API and run any commands.</li>
<li><code>resultBucket</code> and <code>deployBucket</code>: Those S3 buckets will be used the framework to store the results of the extraction and to store the packages .jar file for later deployment on any launched instance. You can create the buckets either via the Web Interface or using the command line tool <code>s3cmd</code>:
<pre><code>apt-get install s3cmd<br>s3cmd --configure<br>s3cmd mb s3://[resultbucketname]<br>s3cmd mb s3://[deploybucketname]</code></pre></li>
<li><code>ec2instancetype</code>: Defines the type of EC2 instances which will be launched. In order to find out which instance type servers your needs best, visit the <a href="https://aws.amazon.com/ec2/instance-types/">ec2 instance type website</a>. Former extractions by the WebDataCommons team where mainly done using <code>c1.xlarge</code> instances.</li>
<li><code>processorClass</code>: This property defines which class is used for the extraction. So far three classes are implemented, which where used to run the former extractions of the WebDataCommons team:
<ul>
<li><code>org.webdatacommons.hyperlinkgraph.processor.WatProcessor</code>, which was used to extract the hyperlink graph from the .wat files of the Spring 2014 Common Crawl dataset.</li>
<li><code>org.webdatacommons.structureddata.processor.ArcProcessor</code>, which was used to extract the RDFa, Microdata and Microformats from the .arc files of 2012 Common Crawl dataset.</li>
<li><code>org.webdatacommons.structureddata.processor.WarcProcessor</code>, which was used to extract the RDFa, Microdata and Microformats from the .warc files of the November 2013 Common Crawl dataset.</li>
</ul>
All these classes implement the <code>org.webdatacommons.framework.processor.FileProcessor</code> interface.
</li>
</ul></li>
<li>Package the WDC Extraction Framework using Maven. Make sure you are using Maven3, as earlier versions will not work. <pre><code>mvn package</code></pre><b>Note</b>: When packaging the project for the first time, a large number of libraries will be downloaded into your .m2 directory, mainly from breda.informatik.uni-mannheim.de, which might take some time.<br>
After successfully packaging the project, there should be a new directory, named <code>target</code> within your root directory of the project. Besides others, this directory should include the packaged .jar file: <code>dpef-*.jar</code></li>
</ol>
</p>
<h3 id="runfirst">2.3 Running your first extraction</h3>
<p>
After you have packaged the code you can use the <code>bin/master</code> bash script to start and steer your extraction. This bash scripts calls functions implemented within the <code>org.webdatacommons.framework.cli.Master</a></code> class. To execute the script you need to make it executable:<pre><code>chmod +x bin/master</code></pre>
By following the commands below, you will first push your the .jar file to S3, than fill you queue with tasks/files which need to be processed and than start a number of EC2 instances which will execute the files. You can monitor the extraction process and after finishing stop all instances again. It also includes commands to collect your results and store it to your local hard drive. <b>Note</b>: Please always take in mind, that the framework will make use of AWS services and that Amazon will charge you for their usage.
<ol>
<li><b>Deploy</b> to upload the JAR to S3:<pre><code>./bin/master deploy --jarfile target/dpef-*.jar</code></pre></li>
<li><b>Queue</b> to fill the extraction queue with the Common Crawl files you want to process: <pre><code>/bin/master queue --bucket-prefix CC-MAIN-2013-48/segments/1386163041297/wet/</code></pre> Please note, that the queue command is just fetching files within one folder. In case you need files located in different folders use the bucket prefix file option of the command. You can also limit the number of files pushed to the queue.</li>
<li><b>Start</b> to launch EC2 extraction instances from the spot market. The command will keep starting instances until it is cancelled, so beware! Also, the price limit has to be given. The current spot prices can be found at http://aws.amazon.com/ec2/spot-instances/#6. A general recommendation is to set this price at about the on-demand instance price. This way, we will benefit from the generally low spot prices without our extraction process being permanently killed. The price limit is given in US$.<pre><code>./bin/master start --worker-amount 10 --pricelimit 0.6</code></pre><b>Note</b>: It may take a while (approx. 10 Minutes) for the instances to become available and start taking tasks from the queue. You can observe the process of the spot requests within the AWS Web Dashboard.</li>
<li><b>Monitor</b> to monitor the process including the number of items in the queue, the approximate time to finish and the number of running/requested instances<pre><code>./bin/master monitor</code></pre></li>
<li><b>Shutdown</b> to kill all worker nodes and terminate the spot instance requests<pre><code>./bin/master shutdown</code></pre></li>
<li><b>Retrieve Data</b> to retrieve all collected data to a local directory<pre><code>./bin/master  retrievedata --destination /some/directory</code></pre></li>
<li><b>Retrieve Stats</b> to retrieve all collected data statistics to a local directory from the Simple DB of AWS<pre><code>./bin/master  retrievestats --destination /some/directory</code></pre></li>
<li><b>Clear Queue</b> to remove all remaining tasks from the queue and delete it<pre><code>./bin/master clearqueue</code></pre></li>
<li><b>Clear Data</b> to remove all collected data from the Simple DB<pre><code>./bin/master cleardata</code></pre></li>
</ol>
For additional information and parameters which might be useful for your task you can have a look in the documentation of the different commands or have a look in the implementation class <code>org.webdatacommons.framework.cli.Master</a></code>.
</p>

<h2 id="customize">3. Customize your extraction</h2>
<p>
In this section we show how to build and run your own extractor. For this task we think about a large number of text files, where each line consists of a number of chars. We are now interested in all lines which only consists of a number. We want to extract those numeric lines and in addition want to have two basic statistics about the each file, namely the total number of lines and the number of lines consisting of only digits.
</p>

<h3 id="processor_prelim">3.1. Preliminaries</h3>
<p>
Building your own extractor is easy. Extractors are called processors within the WDC Extraction Framework and need to implement he <code>FileProcessor.java</code> interface:
<pre>
<tt class="java"><span class="java4">package </span><span class="java10">org.webdatacommons.framework.processor;<br />
</span><span class="java4">import </span><span class="java10">java.nio.channels.ReadableByteChannel;
</span><span class="java4">import </span><span class="java10">java.util.Map;<br />
</span><span class="java4">public interface </span><span class="java10">FileProcessor </span><span class="java8">{<br />
&#xA0; </span><span class="java10">Map&lt;String, String&gt; process</span><span class="java8">(</span><span class="java10">ReadableByteChannel fileChannel,
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; String inputFileKey</span><span class="java8">) </span><span class="java4">throws </span><span class="java10">Exception;<br />
</span><span class="java8">}</span></tt>
</pre>
The interface is fairly simple, as it only contains one method, which receives a <code>ReadableByteChannel</code>, representing the file to process and the files name as input. As result the method returns a <code>Map&lt;String, String&gt;</code> of key value pairs containing statistics of the processed file. The returned key value pairs are written to the AWS Simple DB (see property <code>sdbdatadomain</code> of the <code>dpef.properties</code>). In case you do not need any statistics you can also return an empty <code>Map</code>. 
<br/><br/>
In addition the WDC Extraction Framework offers a context class, named <code>ProcessingNode.java</code> which allows the extending class to make use of contextual settings (e.g. AWS Services to store files). In many cases it makes sense to first have a look into this class before thinking about an own solution.
</p>
<h3 id="processor_custom">3.2. Building your own extractor</h3>
<p>
We create the <code>TextFileProcessor.java</code> which implements the interface and extends the <code>ProcessingNode.java</code> to make use of the context (see line 14).
The class will process a text file, read each line and check if the line consists only of digits. Lines matching this requirement will be written into a new output file.<br/>
The code processes the file in the following way:
<ol>
<li>Initialization of counts (line 18 - 21)</li>
<li>Creating a <code>BufferedReader</code> from the <code>ReadableByteChannel</code>, which makes it easy to read the input file line by line (line 22/23)</li>
<li>Creating a <code>BufferedWriter</code> for a temporal file, which will be deleted as soon as we exit it. This ensures that the hard drive will not be flooded (line 24 - 30)</li>
<li>Processing the file line by line, counting each line and matching each line, if only digits are included (line 31 - 41)</li>
<li>Having the file parsed completely, the output needs to be pushed to S3 using the context from <code>ProcessingNode.java</code> (line 44 - 52)</li>
<li>Finally, we add the statistics of the file (number of lines, number of only digits lines) to the output map, which will be written to AWS Simple DB (line 53 - 56)</li>
</ol>
<pre>
<tt class="java">
1  <span class="java4">package </span><span class="java10">org.webdatacommons.example.processor;

2  </span><span class="java4">import </span><span class="java10">java.io.BufferedReader;
3  </span><span class="java4">import </span><span class="java10">java.io.BufferedWriter;
4  </span><span class="java4">import </span><span class="java10">java.io.File;
5  </span><span class="java4">import </span><span class="java10">java.io.FileWriter;
6  </span><span class="java4">import </span><span class="java10">java.io.InputStreamReader;
7  </span><span class="java4">import </span><span class="java10">java.nio.channels.Channels;
8  </span><span class="java4">import </span><span class="java10">java.nio.channels.ReadableByteChannel;
9  </span><span class="java4">import </span><span class="java10">java.util.HashMap;
10 </span><span class="java4">import </span><span class="java10">java.util.Map;

11 </span><span class="java4">import </span><span class="java10">org.jets3t.service.model.S3Object;

12 </span><span class="java4">import </span><span class="java10">org.webdatacommons.framework.processor.FileProcessor;
13 </span><span class="java4">import </span><span class="java10">org.webdatacommons.framework.processor.ProcessingNode;

14 </span><span class="java4">public class </span><span class="java10">TextFileProcessor </span><span class="java4">extends </span><span class="java10">ProcessingNode </span><span class="java4">implements </span><span class="java10">FileProcessor </span><span class="java8">{
<br />
15 &#xA0; </span><span class="java16">@Override
16 &#xA0; </span><span class="java4">public </span><span class="java10">Map&lt;String, String&gt; process</span><span class="java8">(</span><span class="java10">ReadableByteChannel fileChannel,
17 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; String inputFileKey</span><span class="java8">) </span><span class="java4">throws </span><span class="java10">Exception </span><span class="java8">{

18 &#xA0;&#xA0;&#xA0; </span> <span class="java3">// initialize line count
19 &#xA0;&#xA0;&#xA0; </span><span class="java9">long </span><span class="java10">lnCnt = </span><span class="java7">0</span><span class="java10">;<br />
20 &#xA0;&#xA0;&#xA0; </span><span class="java3">// initialize match count
21 &#xA0;&#xA0;&#xA0; </span><span class="java9">long </span><span class="java10">mCnt = </span><span class="java7">0</span><span class="java10">;<br />
22 &#xA0;&#xA0;&#xA0; </span><span class="java3">// Creating a buffered reader from the input stream - the channel is not compressed
23 &#xA0;&#xA0;&#xA0; </span><span class="java10">BufferedReader br = </span><span class="java4">new </span><span class="java10">BufferedReader</span><span class="java8">(</span><span class="java4">new </span><span class="java10">InputStreamReader</span><br/>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; <span class="java8">(</span><span class="java10">Channels.newInputStream</span><span class="java8">(</span><span class="java10">fileChannel</span><span class="java8">)))</span><span class="java10">;
24 &#xA0;&#xA0;&#xA0; </span><span class="java3">// Create a temporal file for our output
25 &#xA0;&#xA0;&#xA0; </span><span class="java10">File tempOutputFile = File.createTempFile</span><span class="java8">(
26 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java5">&#34;tmp_&#34; </span><span class="java10">+ inputFileKey.replace</span><span class="java8">(</span><span class="java5">&#34;/&#34;</span><span class="java10">, </span><span class="java5">&#34;_&#34;</span><span class="java8">)</span><span class="java10">, </span><span class="java5">&#34;.digitsonly.txt&#34;</span><span class="java8">)</span><span class="java10">;
27 &#xA0;&#xA0;&#xA0; </span><span class="java3">// we delete it on exit - so we do not flood the hard drive
28 &#xA0;&#xA0;&#xA0; </span><span class="java10">tempOutputFile.deleteOnExit</span><span class="java8">()</span><span class="java10">;
29 &#xA0;&#xA0;&#xA0; </span><span class="java3">// Create the writer for the output
30 &#xA0;&#xA0;&#xA0; </span><span class="java10">BufferedWriter bw = </span><span class="java4">new </span><span class="java10">BufferedWriter</span><span class="java8">(</span><span class="java4">new </span><span class="java10">FileWriter</span><span class="java8">(</span><span class="java10">tempOutputFile</span><span class="java8">))</span><span class="java10">;

31 &#xA0;&#xA0;&#xA0; </span><span class="java4">while </span><span class="java8">(</span><span class="java10">br.ready</span><span class="java8">()){
32 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java3">// reading the channel file by file
33 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java10">String line = br.readLine</span><span class="java8">()</span><span class="java10">;
34 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; lnCnt++;
35 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java3">// Check if the line match our pattern
36 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java4">if </span><span class="java8">(</span><span class="java10">line != </span><span class="java4">null </span><span class="java10">&amp;&amp; line.matches</span><span class="java8">(</span><span class="java5">&#34;[0-9]+&#34;</span><span class="java8">)){
37 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java10">mCnt++;
38 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java3">// write the line to the output file
39 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java10">bw.write</span><span class="java8">(</span><span class="java10">line</span><span class="java8">)</span><span class="java10">;
40 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java8">}
41 &#xA0;&#xA0;&#xA0; }
42 &#xA0;&#xA0;&#xA0; </span><span class="java10">br.close</span><span class="java8">()</span><span class="java10">;
43 &#xA0;&#xA0;&#xA0; bw.close</span><span class="java8">()</span><span class="java10">;<br />
44 &#xA0;&#xA0;&#xA0; </span><span class="java3">// Now the file is parsed completely and the output needs to be stored to s3
45 &#xA0;&#xA0;&#xA0; // create an s3 object
46 &#xA0;&#xA0;&#xA0; </span><span class="java10">S3Object dataFileObject = </span><span class="java4">new </span><span class="java10">S3Object</span><span class="java8">(</span><span class="java10">tempOutputFile</span><span class="java8">)</span><span class="java10">;
47 &#xA0;&#xA0;&#xA0; </span><span class="java3">// name the file
48 &#xA0;&#xA0;&#xA0; </span><span class="java10">String outputFileKey = inputFileKey.replace</span><span class="java8">(</span><span class="java5">&#34;/&#34;</span><span class="java10">, </span><span class="java5">&#34;_&#34;</span><span class="java8">) </span><span class="java10">+ </span><span class="java5">&#34;digitsonly.txt&#34;</span><span class="java10">;
49 &#xA0;&#xA0;&#xA0; </span><span class="java3">// set the name
50 &#xA0;&#xA0;&#xA0; </span><span class="java10">dataFileObject.setKey</span><span class="java8">(</span><span class="java10">outputFileKey</span><span class="java8">)</span><span class="java10">;
51 &#xA0;&#xA0;&#xA0; </span><span class="java3">// put the object to the result bucket
52 &#xA0;&#xA0;&#xA0; </span><span class="java10">getStorage</span><span class="java8">()</span><span class="java10">.putObject</span><span class="java8">(</span><span class="java10">getOrCry</span><span class="java8">(</span><span class="java5">&#34;resultBucket&#34;</span><span class="java8">)</span><span class="java10">, dataFileObject</span><span class="java8">)</span><span class="java10">;

53 &#xA0;&#xA0;&#xA0; </span><span class="java3">// create the statistic map (key, value) for this file
54 &#xA0;&#xA0;&#xA0; </span><span class="java10">Map&lt;String, String&gt; map = </span><span class="java4">new </span><span class="java10">HashMap&lt;String, String&gt;</span><span class="java8">()</span><span class="java10">;
55 &#xA0;&#xA0;&#xA0; map.put</span><span class="java8">(</span><span class="java5">&#34;lines_total&#34;</span><span class="java10">, String.valueOf</span><span class="java8">(</span><span class="java10">lnCnt</span><span class="java8">))</span><span class="java10">;
56 &#xA0;&#xA0;&#xA0; map.put</span><span class="java8">(</span><span class="java5">&#34;lines_match&#34;</span><span class="java10">, String.valueOf</span><span class="java8">(</span><span class="java10">mCnt</span><span class="java8">))</span><span class="java10">;
57 &#xA0;&#xA0;&#xA0; </span><span class="java4">return </span><span class="java10">map;
58 &#xA0; </span><span class="java8">}
59 }</span></tt>
</pre>
</p>
<h3 id="package_custom">3.3. Packaging your new extractor</h3>
<p>
As the extractor is done, you simply need to add the processor into the property <code>processorClass</code> within the <code>dpef.properties</code> file and run <code>mvn package</code>.
</p>
<h2 id="costs">4. Costs</h2>
<p>
The usage of the WDC framework is free of charge. Nevertheless, as the framework makes use of services from AWS, Amazon will charge you for the usage.
There are several granting possibilities by Amazon itself, where Amazon supports ideas and projects running within their cloud system with free credits - specially in the education area (see <a href="http://aws.amazon.com/grants/">Amazon Grants</a>).
</p>

<h2 id="license">5. License</h2>

<p>The Web Data Commons extraction framework can be used under the terms of the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache Software License</a>. </p>

<h2 id="feedback">6. Feedback</h2>
<p>Please send questions and feedback to the <a href="http://groups.google.com/group/web-data-commons">Web Data Commons mailing list</a> or post them in our <a href="https://groups.google.com/forum/?fromgroups#!forum/web-data-commons">Web Data Commons Google Group</a>.<br/><br/>
More information about Web Data Commons is found <a href="../index.html">here</a>.</p>

</div>

</div>

<script type="text/javascript">
$('#toc').toc({
    'selectors': 'h2,h3', //elements to use as headings
    'container': '#toccontent', //element to find all selectors in
    'smoothScrolling': true, //enable or disable smooth scrolling on click
    'prefix': 'toc', //prefix for anchor tags and class names
    'highlightOnScroll': true, //add class to heading that is currently in focus
    'highlightOffset': 100, //offset to trigger the next headline
    'anchorName': function(i, heading, prefix) { //custom function for anchor name
        return prefix+i;
    } 
});
</script>

</body>
</html> 
