<!DOCTYPE html>
<html><head><title>WebIsA Database</title>
<link rel="stylesheet" href="http://webdatacommons.org/style.css" type="text/css" media="screen"/>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<style>
.tar {
text-align: right;
}
.rtable 
{
    float: right; 
    padding-left:10px;
        
}
.smalltable, .smalltable TD, .smalltable TH 
{
font-size:9pt; 
}
.table-wrapper {
  position:relative;
}
.table-scroll {
  height:180px;
  overflow:auto;  
  margin-top:-10px;
}

</style>
</head>
<body> 

  <div id="logo" style="text-align:right; background-color: white;">&nbsp;&nbsp;<a href="http://dws.informatik.uni-mannheim.de"><img src="../images/ma-logo.gif" alt="University of Mannheim - Logo"></a></div>


<div id="header">
<h1 style="font-size: 250%;">Web Data Commons - WebIsA Database</h1>
</div>

<div id="tagline">Generating a "IsA" Database out of the Web</div>

<div id="authors">
<a href="http://dws.informatik.uni-mannheim.de/en/people/professors/prof-dr-christian-bizer/">Christian Bizer</a><br />
<a href="http://dws.informatik.uni-mannheim.de/en/people/alumni/prof-dr-kai-eckert/">Kai Eckert</a><br />
<a href="http://dws.informatik.uni-mannheim.de/en/people/researchers/drstefanofaralli/">Stefano Faralli</a><br />
<a href="http://dws.informatik.uni-mannheim.de/en/people/researchers/robert-meusel/">Robert Meusel</a><br />
<a href="http://dws.informatik.uni-mannheim.de/en/people/professors/dr-heiko-paulheim/">Heiko Paulheim</a><br />
<a href="http://dws.informatik.uni-mannheim.de/en/people/professors/profdrsimonepaoloponzetto/">Simone Paolo Ponzetto</a><br />
</div>

<div id="content">
<p>
WebIsADb is a publicly available database containing more than 400 million hypernymy relations we extracted from the CommonCrawl web corpus. 
This collection of relations represents a rich source of knowledge and may be useful for many researchers. We offer the tuple dataset for public download and an application programming interface (API) to help other researchers programmatically query the database.
</p>

<h2 id="news">News</h2>
<ul>
<li><strong>2016-01-01: Paper about the Is-A-Database was accepted at LREC 2016.</strong></li>
</ul>
<h2>Contents</h2>
<div id="toccontent">
<h2 id="about">1. WebIsADb</h2>
<p>Our approach to "isa" relation extraction can be divided in three main steps:

<div class="rtable"> 
<h4>Table 1: The list of patterns used for the tuples extraction<br />
    ($NP_t$ indicates the hyponym and $NP_h$ the hypernym). <br />
    For each pattern we also report the estimated precision and <br/>
    the number of resulting matches.</h4><br />


<div class="table-wrapper">
<table class="smalltable" width="100%">
<tr><th width="220px">Pattern</th><th width="60px">Precision</th><th># match</th></tr>
</table>
<div class="table-scroll">
<table class="smalltable">
<tr style="visibility:hidden"><th width="220px">Pattern</th><th width="60px">Precision</th><th># match</th></tr>
<tr><td>$NP_t$ and any other $NP_h$</td><td class="tar">0.76</td><td class="tar">975,735</td></tr>
<tr><td>$NP_t$ and other $NP_h$</td><td class="tar">0.7</td><td class="tar">45,900,092</td></tr>
<tr><td>$NP_t$ or other $NP_h$</td><td class="tar">0.7</td><td class="tar">13,392,348</td></tr>
<tr><td>$NP_t$ is $adj_{sup}$ $NP_h$</td><td class="tar">0.63</td><td class="tar">6,150,245</td></tr>
<tr><td>$NP_t$ is $adj_{sup}$ most $NP_h$</td><td class="tar">0.63</td><td class="tar">2,286,478</td></tr>
<tr><td>$NP_h$ such as $NP_t$</td><td class="tar">0.58</td><td class="tar">70,337,543</td></tr>
<tr><td>such $NP_h$ as $NP_t$</td><td class="tar">0.58</td><td class="tar">5,755,389</td></tr>
<tr><td>$NP_t$ are a $NP_h$</td><td class="tar">0.57</td><td class="tar">15,141,131</td></tr>
<tr><td>$NP_t$ and some other $NP_h$</td><td class="tar">0.54</td><td class="tar">296,524</td></tr>
<tr><td>$NP_t$ which is called $NP_h$</td><td class="tar">0.5</td><td class="tar">119,317</td></tr>
<tr><td>$NP_t$ are $adj_{sup}$ most $NP_h$</td><td class="tar">0.49</td><td class="tar">860,770</td></tr>
<tr><td>examples of $NP_h$ are $NP_t$</td><td class="tar">0.45</td><td class="tar">267,764</td></tr>
<tr><td>$NP_t$, kinds of $NP_h$</td><td class="tar">0.45</td><td class="tar">4,618,873</td></tr>
<tr><td>$NP_h$ including $NP_t$</td><td class="tar">0.44</td><td class="tar">80,640,885</td></tr>
<tr><td>$NP_t$ is a $NP_h$</td><td class="tar">0.44</td><td class="tar">187,644,160</td></tr>
<tr><td>$NP_h$ other than $NP_t$</td><td class="tar">0.44</td><td class="tar">7,175,087</td></tr>
<tr><td>$NP_t$ were a $NP_h$</td><td class="tar">0.42</td><td class="tar">3,206,238</td></tr>
<tr><td>$NP_t$ are $adj_{sup}$ $NP_h$</td><td class="tar">0.41</td><td class="tar">1,393,484</td></tr>
<tr><td>$NP_t$ was a $NP_h$</td><td class="tar">0.39</td><td class="tar">39,585,428</td></tr>
<tr><td>$NP_t$, one of the $NP_h$</td><td class="tar">0.38</td><td class="tar">4,200,376</td></tr>
<tr><td>$NP_t$ is example of $NP_h$</td><td class="tar">0.36</td><td class="tar">292,706</td></tr>
<tr><td>examples of $NP_h$ is $NP_t$</td><td class="tar">0.33</td><td class="tar">267,021</td></tr>
<tr><td>$NP_h$ e.g. $NP_t$</td><td class="tar">0.33</td><td class="tar">1,973,022</td></tr>
<tr><td>$NP_t$, forms of $NP_h$</td><td class="tar">0.33</td><td class="tar">3,326,957</td></tr>
<tr><td>$NP_t$ like other $NP_h$</td><td class="tar">0.31</td><td class="tar">402,388</td></tr>
<tr><td>$NP_h$ for example $NP_t$</td><td class="tar">0.31</td><td class="tar">2,356,522</td></tr>
<tr><td>$adj_{sup}$ most $NP_h$ is $NP_t$</td><td class="tar">0.31</td><td class="tar">2,999,877</td></tr>
<tr><td>$NP_t$ or the many $NP_h$</td><td class="tar">0.31</td><td class="tar">15,192</td></tr>
<tr><td>$NP_h$ i.e. $NP_t$</td><td class="tar">0.29</td><td class="tar">2,114,793</td></tr>
<tr><td>$NP_h$ which is similar to $NP_t$</td><td class="tar">0.29</td><td class="tar">63,713</td></tr>
<tr><td>$NP_h$ notably $NP_t$</td><td class="tar">0.28</td><td class="tar">1,154,745</td></tr>
<tr><td>$NP_h$ which are similar to $NP_t$</td><td class="tar">0.28</td><td class="tar">17,304</td></tr>
<tr><td>$NP_t$ which is named $NP_h$</td><td class="tar">0.26</td><td class="tar">19,122</td></tr>
<tr><td>$NP_h$ principally $NP_t$</td><td class="tar">0.26</td><td class="tar">455,578</td></tr>
<tr><td>$adj_{sup}$ $NP_h$ is $NP_t$</td><td class="tar">0.25</td><td class="tar">10,360,953</td></tr>
<tr><td>$NP_h$ in particular $NP_t$</td><td class="tar">0.25</td><td class="tar">2,354,596</td></tr>
<tr><td>$NP_h$ example of this is $NP_t$</td><td class="tar">0.25</td><td class="tar">14,237</td></tr>
<tr><td>$NP_h$ among them $NP_t$</td><td class="tar">0.23</td><td class="tar">524,784</td></tr>
<tr><td>$NP_h$ mainly $NP_t$</td><td class="tar">0.22</td><td class="tar">4,792,792</td></tr>
<tr><td>$NP_h$ except $NP_t$</td><td class="tar">0.22</td><td class="tar">9,648,662</td></tr>
<tr><td>$adj_{sup}$ most $NP_h$ are $NP_t$</td><td class="tar">0.21</td><td class="tar">2,357,968</td></tr>
<tr><td>$NP_t$ are examples of $NP_h$</td><td class="tar">0.2</td><td class="tar">2,205,089</td></tr>
<tr><td>$NP_h$ especially $NP_t$</td><td class="tar">0.19</td><td class="tar">20,872,227</td></tr>
<tr><td>$adj_{sup}$ $NP_h$ are $NP_t$</td><td class="tar">0.19</td><td class="tar">3,755,893</td></tr>
<tr><td>$NP_h$ particularly $NP_t$</td><td class="tar">0.19</td><td class="tar">11,656,254</td></tr>
<tr><td>$NP_t$, a kind of $NP_h$</td><td class="tar">0.18</td><td class="tar">1,452,822</td></tr>
<tr><td>$NP_t$, a form of $NP_h$</td><td class="tar">0.18</td><td class="tar">1,127,173</td></tr>
<tr><td>$NP_t$ which sound like $NP_h$</td><td class="tar">0.18</td><td class="tar">32,730</td></tr>
<tr><td>$NP_h$ examples of this are $NP_t$</td><td class="tar">0.18</td><td class="tar">1,515</td></tr>
<tr><td>$NP_t$ sort of $NP_h$</td><td class="tar">0.18</td><td class="tar">7,884,398</td></tr>
<tr><td>$NP_h$ types $NP_t$</td><td class="tar">0.17</td><td class="tar">11,080,276</td></tr>
<tr><td>$NP_h$ compared to $NP_t$ </td><td class="tar">0.17</td><td class="tar">346,525</td></tr>
<tr><td>$NP_h$ mostly $NP_t$</td><td class="tar">0.16</td><td class="tar">8,383,063</td></tr>
<tr><td>compare $NP_t$ with $NP_h$ </td><td class="tar">0.16</td><td class="tar">340,636</td></tr>
<tr><td>$NP_t$, one of those $NP_h$</td><td class="tar">0.15</td><td class="tar">99,241</td></tr>
<tr><td>$NP_t$, one of these $NP_h$</td><td class="tar">0.13</td><td class="tar">53,235</td></tr>
<tr><td>$NP_t$ which look like $NP_h$</td><td class="tar">0.13</td><td class="tar">68,945</td></tr>
<tr><td>$NP_h$ whether $NP_t$ or </td><td class="tar">0.13</td><td class="tar">2,800,349</td></tr>
</table>
</div>  
</div>  
</div>  
<ol>
<li> <h3> WebDataCommons framework</h3><br />
To extract a large collection of "isa" relations from the Web, we decided to rely on the largest publicly available Web corpus i.e. the crawl corpora provided by the <a href="http://commoncrawl.org">CommonCrawl Foundation</a>. The original corpus contains over 2.1 billion crawled web pages, consisting of over 38000 Web ARChive, ISO 28500:2009 (WARC) files with a total packed size of 168TB. 
To efficiently extract "isa" relations from the crawl corpora, our implementation of relations extraction directly synchronizes with the framework of the <a href="http://webdatacommons.org/framework/">WebDataCommons project</a>;
</li>
<li> <h3>Extraction and filtering</h3><br/>
The extraction of "isa" relations from text is based on Hearst-like patterns. We collected a set of 59 patterns (see Table 1 for the full list).
The patterns identified are then translated into regular expressions which we use to match with the incoming text. 
In order to test the quality of the above defined regular expressions, we extracted a random 1% portion of the entire corpus and analyzed 100 matches per pattern. With that evaluation, we estimated the precision of each pattern, as shown in Table 1. Patterns with a very low precision have then been excluded before performing the subsequent steps.
Since both the Web and the extraction phase are sources of noise, some post-processing and filtering is required. To facilitate a sensible trade-off between coverage and precision, we try to remove only the obvious noise, while keeping as much coverage as possible. This strategy comes from the idea that some task may need ``less precise'' but ``more covering'' data. For supporting use cases where more precision is required, we provide metadata for each tuple, which allows for additional filtering techniques on the client side. As basic filtering techniques, we i) remove duplicates i.e. tuples that occur more than once under the same pay level domain are removed; ii) transform all the capital letters to lower case and removed all leading and trailing punctuations; iii) remove all quotation marks and apostrophes, since apostrophes are frequently used as replacement for quotation marks.
The extraction and filtering of the tuples took around 2,200 computing hours and was run using 100 servers in parallel in less than 24 hours;
</li>
<li><h3>Indexing</h3><br />
To store and access all the extracted relations we created a <a href="https://www.mongodb.org/">MongoDB</a> database.
</li>
</ol>
</p>
<h2 id="resources">2. Resources</h2>
<p>Description.</p>
<a href="https://docs.mongodb.org/manual/tutorial/backup-and-restore-tools">Restore a MongoDB database</a>
</div>
<script type="text/javascript">
$('#toc').toc({
    'selectors': 'h2,h3', //elements to use as headings
    'container': '#toccontent', //element to find all selectors in
    'smoothScrolling': true, //enable or disable smooth scrolling on click
    'prefix': 'toc', //prefix for anchor tags and class names
    'highlightOnScroll': true, //add class to heading that is currently in focus
    'highlightOffset': 100, //offset to trigger the next headline
    'anchorName': function(i, heading, prefix) { //custom function for anchor name
        return prefix+i;
    }
});
</script>
</body>
</html> 
