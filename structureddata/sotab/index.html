<!DOCTYPE html>
<html>

<head>
	<title>WDC Schema.org Table Annotation Benchmark</title>
	<link rel='stylesheet' href='http://webdatacommons.org/style.css' type='text/css' media='screen' />
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<style>
		.tar {
			text-align: right;
		}

		.rtable {
			float: right;
			padding-left: 10px;
		}

		.smalltable,
		.smalltable TD,
		.smalltable TH {
			font-size: 9pt;
		}

		.tab {
			overflow: hidden;
			border: 1px solid #ccc;
			background-color: #eaf3fa;
			clear: both;
			padding-left: 25px;
			width: 650px;
		}

		.tab button {
			background-color: inherit;
			float: left;
			border: none;
			outline: none;
			cursor: pointer;
			padding: 15px 60px;
			transition: 0.3s;
		}

		.tab button:hover {
			background-color: #ddd;
		}

		.tab button.active {
			background-color: #ccc;
		}

		.tabcontent {
			display: none;
			padding: 6px 12px;
			border-top: none;
			animation: fadeEffect 1s;
			width: 500px
		}

		.table-wrapper {
			position: relative;
		}

		.table-scroll {
			height: 240px;
			overflow: auto;
			margin-top: -10px;
		}

		.show {
			display: block;
		}

		.no-show {
			display: none;
		}

		caption {
			caption-side: top;
			font-style: italic;
		}

		td[scope="mergedcol"] {
			text-align: center;
		}

		tr.bordered {
			border-bottom: 1px solid #000;
		}

		hr {
			width: 50%;
			margin: 20px 0;
			/* This leaves 10px margin on left and right. If only right margin is needed try margin-right: 10px; */
		}

    .tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}
    .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
    .tg .tg-ixdq{border-color:inherit;font-weight:bold;position:-webkit-sticky;position:sticky;text-align:center;top:-1px;
      vertical-align:middle;will-change:transform}
    .tg .tg-mkpc{border-color:inherit;font-weight:bold;position:-webkit-sticky;position:sticky;text-align:left;top:-1px;
      vertical-align:middle;will-change:transform}
    .tg .tg-d459{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-kyy7{background-color:#f9f9f9;border-color:inherit;text-align:center;vertical-align:middle}
    .tg .tg-h2b0{background-color:#FFF;border-color:inherit;color:#333;text-align:center;vertical-align:middle}
    .tg-sort-header::-moz-selection{background:0 0}
    .tg-sort-header::selection{background:0 0}.tg-sort-header{cursor:pointer}
    .tg-sort-header:after{content:'';float:right;margin-top:7px;border-width:0 5px 5px;border-style:solid;
      border-color:#404040 transparent;visibility:hidden}
    .tg-sort-header:hover:after{visibility:visible}
    .tg-sort-asc:after,.tg-sort-asc:hover:after,.tg-sort-desc:after{visibility:visible;opacity:.4}
    .tg-sort-desc:after{border-bottom:none;border-width:5px 5px 0}@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}

    .tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}
    .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
    .tg .tg-baqh{text-align:center;vertical-align:top}
    .tg .tg-zyik{border-color:inherit;font-weight:bold;position:-webkit-sticky;position:sticky;text-align:center;top:-1px;
      vertical-align:top;will-change:transform}
    .tg .tg-ixdq{border-color:inherit;font-weight:bold;position:-webkit-sticky;position:sticky;text-align:center;top:-1px;
      vertical-align:middle;will-change:transform}
    .tg .tg-yy5h{background-color:#F0F0F0;border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-o939{background-color:#f0f0f0;border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-ufyq{background-color:#f0f0f0;font-weight:bold;text-align:center;vertical-align:top}
    .tg .tg-asv9{background-color:#F0F0F0;border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}
    .tg .tg-d459{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-dzk6{background-color:#f9f9f9;text-align:center;vertical-align:top}
    .tg .tg-kyy7{background-color:#f9f9f9;border-color:inherit;text-align:center;vertical-align:middle}
    .tg-sort-header::-moz-selection{background:0 0}
    .tg-sort-header::selection{background:0 0}.tg-sort-header{cursor:pointer}
    .tg-sort-header:after{content:'';float:right;margin-top:7px;border-width:0 5px 5px;border-style:solid;
      border-color:#404040 transparent;visibility:hidden}
    .tg-sort-header:hover:after{visibility:visible}
    .tg-sort-asc:after,.tg-sort-asc:hover:after,.tg-sort-desc:after{visibility:visible;opacity:.4}
    .tg-sort-desc:after{border-bottom:none;border-width:5px 5px 0}@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}

		.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;display:inline-block;margin-right:50px;}
    .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-pl4i{background-color:#f0f0f0;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-j1i3{border-color:inherit;position:-webkit-sticky;position:sticky;text-align:left;top:-1px;vertical-align:top;
      will-change:transform}
    .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
    .tg .tg-ixdq{border-color:inherit;font-weight:bold;position:-webkit-sticky;position:sticky;text-align:center;top:-1px;
      vertical-align:middle;will-change:transform}
    .tg .tg-yy5h{background-color:#F0F0F0;border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-o939{background-color:#f0f0f0;border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-45e1{background-color:#f0f0f0;border-color:inherit;font-weight:bold;text-align:left;vertical-align:middle}
    .tg .tg-nrix{text-align:center;vertical-align:middle}
    .tg .tg-d459{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-kyy7{background-color:#f9f9f9;border-color:inherit;text-align:center;vertical-align:middle}
    .tg .tg-57iy{background-color:#f9f9f9;text-align:center;vertical-align:middle}
    .tg-sort-header::-moz-selection{background:0 0}
    .tg-sort-header::selection{background:0 0}.tg-sort-header{cursor:pointer}
    .tg-sort-header:after{content:'';float:right;margin-top:7px;border-width:0 5px 5px;border-style:solid;
      border-color:#404040 transparent;visibility:hidden}
    .tg-sort-header:hover:after{visibility:visible}
    .tg-sort-asc:after,.tg-sort-asc:hover:after,.tg-sort-desc:after{visibility:visible;opacity:.4}
    .tg-sort-desc:after{border-bottom:none;border-width:5px 5px 0}@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}
				
		@keyframes fadeEffect {
			from {
				opacity: 0;
			}

			to {
				opacity: 1;
			}
		}
	</style>
	
	<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
	<script type="text/javascript" src="../../jquery.toc.min.js"></script>
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-30248817-1']);
		_gaq.push(['_trackPageview']);

		(function () {
			var ga = document.createElement('script');
			ga.type = 'text/javascript';
			ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0];
			s.parentNode.insertBefore(ga, s);
		})();
	</script>
	<script type="application/ld+json">
		{
			"@context": "http://schema.org/",
			"@type": "Dataset",
			"name": "Web Data Commons - Schema.org Table Corpus",
			"description": "The product dataset consists of 20 million pairs of product offers referring to the same products. The offers were extracted from 43 thousand e-shops which provide schema.org annotations including some form of product ID such as a GTIN or MPN. We also created a gold standard by manually verifying 4400 pairs of offers belonging to four different product categories.",
			"url": "http://webdatacommons.org/structureddata/schemaorgtables/index.html",
			"keywords": [
				"table corpus",
				"product corpus",
				"large scale",
				"product matching",
				"entity matching"
			],
			"creator": [
				{
					"@type": "Person",
					"url": "https://www.uni-mannheim.de/dws/people/researchers/phd-students/ralph-peeters/"
					"name": "Ralph Peeters"
				},
				{
					"@type": "Person",
					"url": "https://www.uni-mannheim.de/dws/people/professors/prof-dr-christian-bizer/",
					"name": "Christian Bizer"
				}
			],
			"distribution": [{
				"@type": "DataDownload",
				"fileFormat": [
					"json"
				],
				//TODO: Change once final
				"contentUrl": "http://webdatacommons.org/structureddata/schemaorgtables/index.html"
			}],
			"citation": [

			]
		}
	</script>

<script charset="utf-8">var TGSort=window.TGSort||function(n){"use strict";function r(n){return n?n.length:0}function t(n,t,e,o=0){for(e=r(n);o<e;++o)t(n[o],o)}function e(n){return n.split("").reverse().join("")}function o(n){var e=n[0];return t(n,function(n){for(;!n.startsWith(e);)e=e.substring(0,r(e)-1)}),r(e)}function u(n,r,e=[]){return t(n,function(n){r(n)&&e.push(n)}),e}var a=parseFloat;function i(n,r){return function(t){var e="";return t.replace(n,function(n,t,o){return e=t.replace(r,"")+"."+(o||"").substring(1)}),a(e)}}var s=i(/^(?:\s*)([+-]?(?:\d+)(?:,\d{3})*)(\.\d*)?$/g,/,/g),c=i(/^(?:\s*)([+-]?(?:\d+)(?:\.\d{3})*)(,\d*)?$/g,/\./g);function f(n){var t=a(n);return!isNaN(t)&&r(""+t)+1>=r(n)?t:NaN}function d(n){var e=[],o=n;return t([f,s,c],function(u){var a=[],i=[];t(n,function(n,r){r=u(n),a.push(r),r||i.push(n)}),r(i)<r(o)&&(o=i,e=a)}),r(u(o,function(n){return n==o[0]}))==r(o)?e:[]}function v(n){if("TABLE"==n.nodeName){for(var a=function(r){var e,o,u=[],a=[];return function n(r,e){e(r),t(r.childNodes,function(r){n(r,e)})}(n,function(n){"TR"==(o=n.nodeName)?(e=[],u.push(e),a.push(n)):"TD"!=o&&"TH"!=o||e.push(n)}),[u,a]}(),i=a[0],s=a[1],c=r(i),f=c>1&&r(i[0])<r(i[1])?1:0,v=f+1,p=i[f],h=r(p),l=[],g=[],N=[],m=v;m<c;++m){for(var T=0;T<h;++T){r(g)<h&&g.push([]);var C=i[m][T],L=C.textContent||C.innerText||"";g[T].push(L.trim())}N.push(m-v)}t(p,function(n,t){l[t]=0;var a=n.classList;a.add("tg-sort-header"),n.addEventListener("click",function(){var n=l[t];!function(){for(var n=0;n<h;++n){var r=p[n].classList;r.remove("tg-sort-asc"),r.remove("tg-sort-desc"),l[n]=0}}(),(n=1==n?-1:+!n)&&a.add(n>0?"tg-sort-asc":"tg-sort-desc"),l[t]=n;var i,f=g[t],m=function(r,t){return n*f[r].localeCompare(f[t])||n*(r-t)},T=function(n){var t=d(n);if(!r(t)){var u=o(n),a=o(n.map(e));t=d(n.map(function(n){return n.substring(u,r(n)-a)}))}return t}(f);(r(T)||r(T=r(u(i=f.map(Date.parse),isNaN))?[]:i))&&(m=function(r,t){var e=T[r],o=T[t],u=isNaN(e),a=isNaN(o);return u&&a?0:u?-n:a?n:e>o?n:e<o?-n:n*(r-t)});var C,L=N.slice();L.sort(m);for(var E=v;E<c;++E)(C=s[E].parentNode).removeChild(s[E]);for(E=v;E<c;++E)C.appendChild(s[v+L[E-v]])})})}}n.addEventListener("DOMContentLoaded",function(){for(var t=n.getElementsByClassName("tg"),e=0;e<r(t);++e)try{v(t[e])}catch(n){}})}(document)</script>
				

</head>

<body>
	<div id="logo" style="text-align:right; background-color: white;">&nbsp;&nbsp;<a
			href="http://dws.informatik.uni-mannheim.de"><img src="../../images/ma-logo.gif"
				alt="University of Mannheim - Logo"></a></div>
	<div id="header">
		<h1 style="font-size: 250%;"> Web Data Commons - Schema.org Table Annotation Benchmark </h1>
	</div>
	<div id="authors">
    <a href="https://www.uni-mannheim.de/dws/people/researchers/phd-students/keti-korini/">Keti Korini</a><br>
		<a href="https://www.uni-mannheim.de/dws/people/researchers/phd-students/ralph-peeters/">Ralph Peeters</a><br>
		<a href="https://www.uni-mannheim.de/dws/people/professors/prof-dr-christian-bizer/">Christian Bizer</a><br />
	</div>
	<div id="content">
		<p>
			This page provides the WDC Schema.org <a href="https://paperswithcode.com/task/table-annotation">Table Annotation</a> Benchmark (SOTAB) for public download. SOTAB features two tasks: The goal of the <a href="https://paperswithcode.com/task/column-type-annotation">Column Type Annotation (CTA)</a> task is to annotate the columns of a table with 94 <a href="https://schema.org/">Schema.org</a> types, such as telephone, duration, Place, or Organization. The goal of the <a href="https://paperswithcode.com/task/columns-property-annotation">Columns Property Annotation (CPA)</a> task is to annotate pairs of table columns with one out of 181 <a href="https://schema.org/">Schema.org</a> properties such gtin13, startDate, priceValidUntil, or recipeIngredient. 
			The benchmark consists of 46,090 tables with are split into training-, validation- and test sets for both tasks. The tables cover 17 popular Schema.org classes such as Product, LocalBusiness, Event, or JobPosting and originate from the <a href="http://webdatacommons.org/structureddata/schemaorgtables/">Schema.org Table Corpus</a>.
	       </p>
<h2 id="news">News</h2>
		<ul>
			<li><strong>2022-08:</strong> Initial version of the WDC Schema.org Table Annotation Benchmark released.</li>
		</ul>
		<h2>Contents</h2>
		<ul>
			<li class="toc-h2 toc-active">
				<a href="#toc1"> 1. Introduction</a>
			</li>
			<li class="toc-h2 toc-active">
				<a href="#toc2"> 2. Schema.org Table Corpus</a>
				<ul>
					<li>
            <a href="#toc2.1"> 2.1. Datasets Creation</a>
					</li>
          <li><a href="#toc2.3"> 2.2. Annotation Approach</a> </li>
					<li><a href="#toc2.2"> 2.3. Datasets Profiling</a> </li>
                                        
				</ul>
			</li>

                        <li class="toc-h2 toc-active">
				<a href="#toc5"> 3. Challenges </a>
			</li>

      <li class="toc-h2 toc-active">
				<a href="#toc7"> 4. Baselines </a>
			</li>
			
			<li class="toc-h2 toc-active">
				<a href="#toc3"> 5. Download </a>
			</li>
			<li class="toc-h2 toc-active">
				<a href="#toc4"> 6. References </a>
			<li class="toc-h2 toc-active"><a href="#toc5">7. Other Table Corpora </a>
			</li>
		</ul>

		<span id="toc1"></span>




<h2>1. Introduction</h2>
<p>
        <a href="https://paperswithcode.com/task/table-annotation">Table annotation</a> is the task of annotating a table with terms/concepts from knowledge graph, database schema or other vocabulary. 
        Table annotation consists of five different tasks: column type annotation (CTA), columns property annotation (CPA), cell
        entity annotation (CEA), row annotation, and table type detection. Understanding the semantics of table elements is beneficial for tasks such as data discovery and data integration [ref]. Table annotation has attracted <a href="https://paperswithcode.com/task/table-annotation">quite some attention in the research community</a> in recent years and there are active benchmarking campaigns on table annotation, such as <a href="https://www.cs.ox.ac.uk/isg/challenges/sem-tab/">SemTab</a>. 
        <span id="toc2"></span>
</p>

<p>
  In order to compare the performance of table annotation methods, banchmark datasets covering a wide range of topics are needed. The aim of this work is to create 
  and make available datasets for benchmarking two of the table annotation tasks: column type annotation (CTA) and column pair annotation (CPA). CTA is the task of annotating 
  table columns with the type of entities present in the column, while CPA aims to annotate the relationship between the main column of a table (usually the leftmost) with another
  column in the table. The datasets are build by using tables from the <a href="http://webdatacommons.org/structureddata/schemaorgtables/">Schema.org Table Corpus</a>.
</p>

<h2>2. Schema.org Table Corpus </h2>
<p>
        The original <a href="http://webdatacommons.org/structureddata/schemaorgtables/">Schema.org Table Corpus</a> used for the creation of this benchmark was created using schema.org data from the December 2020 version of the <a href="http://webdatacommons.org/structureddata/2020-12/stats/schema_org_subsets.html">Web Data Commons Microdata and JSON-LD corpus</a>. The Schema.org Table Corpus consists of 4.2 million relational tables covering 43 schema.org classes.
Each table in the Schema.org Table Corpus contains the descriptions of all entities of a specific Schema.org class that were extracted for a specific host, e.g. movie records from imdb.com or product records from ebay.com or rakuten.co.jp. 
All extracted entities of one Schema.org class are collected per host and subsequently passed through a pipeline consisting of three steps, (i) Attribute extraction, (ii) Removal of listing pages and sparse entities, and (iii) content-based deduplication. The result of the pipeline is the final Schema.org table for that specific host. 
As the Attribute extraction step is the most relevant for the creation of this benchmark, it is explained in detail in the following: First, all Schema.org attributes and their values are extracted for each of the collected entities. If an attribute contains a sub-entity instead of a literal value, these are extracted as well and presented as a list in the final table. For the attributes of sub-entities only literal values are considered. The output of this first step are tables (one per host) containing all entities of the current class and their attributes down to the second attribute level, which are the base for the creation of the CPA and CTA benchmarks.
Steps (ii) and (iii) of the Schema.org Table Corpus creation pipeline deal with cleansing the created tables by removing sparse entities deriving from listing pages and general deduplication of exact matching rows in the final tables. Following the cleansing steps, only attributes with a density of at least 25% are retained per table. For more information about these steps and the general Schema.org Table Corpus creation process, statistics files and download links, please visit the <a href="http://webdatacommons.org/structureddata/schemaorgtables/">Schema.org Table Corpus website</a>. 
  
</p>


<h3>2.1. Datasets Creation</h3>
<p>
  For the creation of the datasets, the top100 and minimum3 sets of tables were considered and tables belonging to the 17 classes with the most columns and tables were selected. 
  Afterwards, some steps were taken to select tables for CTA and CPA: 
  (i) Language Detection, (ii) Filtering out the incorrect uses of Schema.org properties, (iii) Expansion of dictionary columns, (iv) Filtering based on number of columns. These steps
  are explained more in detail below.
</p>

<p>
  <b>(i) Language Detection: </b> In this step, the purpose was to filter out tables and rows that were not in the English language. The fastText model was used for this. The <i>description</i> column
  and <i>disambiguatingDescription</i> column were chosen to check the language of a table, as they are longer and contain more text that can indicate the language used. If a table did
  not have one of these columns the concatenation of all other columns was used to determine the language. If the values were predicted to be in English with a confidence of at least 50%,
  the tables were labeled as English tables, otherwise the rows are individually checked and, following the same rule, rows with an English confidence of more than 50% are kept while the
  other rows are discraded.
</p>

<p>
  <b>(ii) Filtering out the incorrect uses of Schema.org properties: </b> As the original corpus was extracted by websites where the website owners annotate their content themselves,
  some errors regarding the usage of Schema.org types and properties can occur. For this reason, the column names (which refer to schema.org properties) were checked for compliance to
  the Schema.org vocabulary. All columns that were incorrectly used were discarded.
</p>

<p>
  <b>(iii) Filtering out tables based on number of columns: </b> In this step, tables that had less than 10 rows and less than 3 columns were filtered out. Furthermore, to insure that
  enough columns are available for each Schema.org property, columns that belonged to properties with less than 50 columns in total across each class were filtered out.
</p>

<p>
  <b>(iv) Expansion of dictionary columns: </b> The original Schema.org Table Corpus tables include columns where the values are dictionaries describing entities which also have
  sub-schema.org properties. These sub-properties of each row were divided and grouped together and they were again checked if they were correctly used according to the Schema.org vocabulary.
  Finally, the correct sub-properties were expanded into new columns in the table. The old columns containing these entities were removed. Some exceptions from the expansion step include
  columns such as <i>image</i>, <i>url</i>, <i>photo</i> and <i>video</i>.
  An example of an original table and its expanded table is shown in <i>Figure 1</i>.
</p>

<p>(IMAGE: Example original table/expanded table->final table)</p>

<p>
  <b>(v) Including challenging columns: </b> In the selection phase of the tables, columns that were missing some row values, columns that are similar or dissimilar and could be challenging to
  make a decision on and columns that had different value formats were found and added to the final selection of the tables.
</p>

<span id="toc2.2"></span>
<h3>2.2. Annotation Approach</h3>
<p>
  To annotate the columns from the final selected tables for CTA and CPA Schema.org vocabulary was used. Column pairs in the CPA problem were annotated using Schema.org properties. In the
  CTA task columns were annotated using Schema.org types. These types were derived from the expected types of a Schema.org property value. If the case where the expected type of a column
  can be multiple, one was chosen based on the values of the column. If a decision could not be made, the most general type of the multiple types was used.
</p> 

 
			
<span id="toc2.3"></span>
<h3>2.3. Corpus Profiling</h3>
<p>
  The SOTAB consists of <b> 46,090 tables</b> for CPA and <b>43,150</b> tables for CTA, distributed across 17 schema.org classes. The tables do not include any metadata, i.e. no table headers 
  and no table captions are available. Fixed training, validation and testing splits are provided.
        
<em>Table 1</em> gives an overview of the number of tables per class and their median columns for CTA and CPA and for each of their splits.
</p> 
			 


<div class="tg-wrap"><table id="tg-tKOYO" class="tg">
<caption>Table 1: Number of tables, rows and median rows per table for CPA and CTA tables</caption>
<thead>
  <tr>
    <th class="tg-j1i3"></th>
    <th class="tg-ixdq" colspan="8">Column Pair Annotation</th>
    <th class="tg-ixdq" colspan="8">Column Type Annotation</th>
  </tr>
</thead>
<tbody>
<tr>
    <th class="tg-j1i3"></th>
    <th class="tg-ixdq" colspan="2">Overall</th>
    <th class="tg-ixdq" colspan="2">Training Set</th>
    <th class="tg-ixdq" colspan="2">Validation Set</th>
    <th class="tg-ixdq" colspan="2">Testing Set</th>

    <th class="tg-ixdq" colspan="2">Overall</th>
    <th class="tg-ixdq" colspan="2">Training Set</th>
    <th class="tg-ixdq" colspan="2">Validation Set</th>
    <th class="tg-ixdq" colspan="2">Testing Set</th>
  </tr>
  <tr>
    <td class="tg-45e1">Schema.org Class</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>

    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
  </tr>
  <tr>
    <td class="tg-lboi">Overall</td>
    <td class="tg-9wq8"> 46,090 </td>
    <td class="tg-9wq8"> 169,515 </td>
    <td class="tg-9wq8"> 34,023 </td>
    <td class="tg-9wq8"> 127,752 </td>
    <td class="tg-9wq8"> 4,324 </td>
    <td class="tg-9wq8"> 16,782 </td>
    <td class="tg-9wq8"> 7,743 </td>
    <td class="tg-9wq8"> 24,981 </td>

    <td class="tg-9wq8"> 43,150 </td>
    <td class="tg-9wq8"> 175,468 </td>
    <td class="tg-9wq8"> 32,713 </td>
    <td class="tg-9wq8"> 135,641 </td>
    <td class="tg-9wq8"> 4,245 </td>
    <td class="tg-9wq8"> 17,990 </td>
    <td class="tg-9wq8"> 6,192 </td>
    <td class="tg-9wq8"> 21,837 </td>
  </tr>
  <tr>
    <td class="tg-lboi">Product</td>
    <td class="tg-9wq8"> 12,024 </td>
    <td class="tg-9wq8"> 28,434 </td>
    <td class="tg-9wq8"> 9,275 </td>
    <td class="tg-9wq8"> 21,387 </td>
    <td class="tg-9wq8"> 996 </td>
    <td class="tg-9wq8"> 2,379 </td>
    <td class="tg-9wq8"> 1,753 </td>
    <td class="tg-9wq8"> 4,668 </td>

    <td class="tg-9wq8">  </td>
    <td class="tg-9wq8">  </td>
    <td class="tg-9wq8"> 7,723 </td>
    <td class="tg-9wq8"> 19,921 </td>
    <td class="tg-9wq8"> 850 </td>
    <td class="tg-9wq8"> 1,907 </td>
    <td class="tg-9wq8"> 1,157 </td>
    <td class="tg-9wq8"> 3,524 </td>
  </tr>
  <tr>
    <td class="tg-d459">Event</td>
    <td class="tg-9wq8"> 6,901 </td>
    <td class="tg-9wq8"> 21,295 </td>
    <td class="tg-9wq8"> 5,143 </td>
    <td class="tg-9wq8"> 16,781 </td>
    <td class="tg-9wq8"> 740 </td>
    <td class="tg-9wq8"> 1,986 </td>
    <td class="tg-9wq8"> 1,018 </td>
    <td class="tg-9wq8"> 2,528 </td>

    <td class="tg-9wq8">  </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 4,570 </td>
    <td class="tg-9wq8"> 15,461 </td>
    <td class="tg-9wq8"> 620 </td>
    <td class="tg-9wq8"> 2,351 </td>
    <td class="tg-9wq8"> 833 </td>
    <td class="tg-9wq8"> 2,665 </td>
  </tr>
  <tr>
    <td class="tg-lboi">Recipe</td>
    <td class="tg-9wq8"> 1,103 </td>
    <td class="tg-9wq8"> 24,078 </td>
    <td class="tg-9wq8"> 3,028 </td>
    <td class="tg-9wq8"> 17,510 </td>
    <td class="tg-9wq8"> 369 </td>
    <td class="tg-9wq8"> 2,288 </td>
    <td class="tg-9wq8"> 734 </td>
    <td class="tg-9wq8"> 4,280 </td>

    <td class="tg-9wq8">  </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 2,568 </td>
    <td class="tg-9wq8"> 18,684 </td>
    <td class="tg-9wq8"> 335 </td>
    <td class="tg-9wq8"> 2,301 </td>
    <td class="tg-9wq8"> 450 </td>
    <td class="tg-9wq8"> 2,675 </td>
  </tr>
  <tr>
    <td class="tg-d459">Person</td>
    <td class="tg-9wq8"> 3,946 </td>
    <td class="tg-9wq8"> 13,267 </td>
    <td class="tg-9wq8"> 2,697 </td>
    <td class="tg-9wq8"> 9,652 </td>
    <td class="tg-9wq8"> 313 </td>
    <td class="tg-9wq8"> 1,157 </td>
    <td class="tg-9wq8"> 936 </td>
    <td class="tg-9wq8"> 2,458 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 1,525 </td>
    <td class="tg-9wq8"> 7,087 </td>
    <td class="tg-9wq8"> 138 </td>
    <td class="tg-9wq8"> 524 </td>
    <td class="tg-9wq8"> 324 </td>
    <td class="tg-9wq8"> 956 </td>
  </tr>
  <tr>
    <td class="tg-lboi">MusicRecording</td>
    <td class="tg-9wq8"> 508 </td>
    <td class="tg-9wq8"> 1,633 </td>
    <td class="tg-9wq8"> 323 </td>
    <td class="tg-9wq8"> 1,115 </td>
    <td class="tg-9wq8"> 42 </td>
    <td class="tg-9wq8"> 160 </td>
    <td class="tg-9wq8"> 143 </td>
    <td class="tg-9wq8"> 358 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 2,279</td>
    <td class="tg-9wq8"> 5,899 </td>
    <td class="tg-9wq8"> 248 </td>
    <td class="tg-9wq8"> 815 </td>
    <td class="tg-9wq8"> 314 </td>
    <td class="tg-9wq8"> 895 </td>
  </tr>
  <tr>
    <td class="tg-d459">Place</td>
    <td class="tg-9wq8"> 1,416 </td>
    <td class="tg-9wq8"> 7,432 </td>
    <td class="tg-9wq8"> 1,194 </td>
    <td class="tg-9wq8"> 6,530 </td>
    <td class="tg-9wq8"> 87 </td>
    <td class="tg-9wq8"> 466 </td>
    <td class="tg-9wq8"> 135 </td>
    <td class="tg-9wq8"> 436 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 670 </td>
    <td class="tg-9wq8"> 4,506 </td>
    <td class="tg-9wq8"> 61 </td>
    <td class="tg-9wq8"> 429 </td>
    <td class="tg-9wq8"> 262 </td>
    <td class="tg-9wq8"> 721 </td>
  </tr>
  <tr>
    <td class="tg-lboi">LocalBusiness</td>
    <td class="tg-9wq8"> 5,899 </td>
    <td class="tg-9wq8"> 16,945 </td>
    <td class="tg-9wq8"> 4,584 </td>
    <td class="tg-9wq8"> 12,402 </td>
    <td class="tg-9wq8"> 561 </td>
    <td class="tg-9wq8"> 2,038 </td>
    <td class="tg-9wq8"> 754 </td>
    <td class="tg-9wq8"> 2,505 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 3,852 </td>
    <td class="tg-9wq8"> 11,534 </td>
    <td class="tg-9wq8"> 540 </td>
    <td class="tg-9wq8"> 1,861 </td>
    <td class="tg-9wq8"> 671 </td>
    <td class="tg-9wq8"> 2,290 </td>
  </tr>
  <tr>
    <td class="tg-d459">CreativeWork</td>
    <td class="tg-9wq8"> 2,073 </td>
    <td class="tg-9wq8"> 8,859 </td>
    <td class="tg-9wq8"> 1,397 </td>
    <td class="tg-9wq8"> 6,453 </td>
    <td class="tg-9wq8"> 225 </td>
    <td class="tg-9wq8"> 1,063 </td>
    <td class="tg-9wq8"> 451 </td>
    <td class="tg-9wq8"> 1,343 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 601 </td>
    <td class="tg-9wq8"> 3,566 </td>
    <td class="tg-9wq8"> 269 </td>
    <td class="tg-9wq8"> 1,554 </td>
    <td class="tg-9wq8"> 353 </td>
    <td class="tg-9wq8"> 1,218 </td>
  </tr>
  <tr>
    <td class="tg-lboi">MusicAlbum</td>
    <td class="tg-9wq8"> 570 </td>
    <td class="tg-9wq8"> 1,486 </td>
    <td class="tg-9wq8"> 397 </td>
    <td class="tg-9wq8"> 1,027 </td>
    <td class="tg-9wq8"> 53 </td>
    <td class="tg-9wq8"> 154 </td>
    <td class="tg-9wq8"> 120 </td>
    <td class="tg-9wq8"> 305 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 699 </td>
    <td class="tg-9wq8"> 2,369 </td>
    <td class="tg-9wq8"> 122 </td>
    <td class="tg-9wq8"> 512 </td>
    <td class="tg-9wq8"> 116 </td>
    <td class="tg-9wq8"> 494 </td>
  </tr>
  <tr>
    <td class="tg-d459">Movie</td>
    <td class="tg-9wq8"> 1,852 </td>
    <td class="tg-9wq8"> 10,900 </td>
    <td class="tg-9wq8"> 1,358 </td>
    <td class="tg-9wq8"> 8,544 </td>
    <td class="tg-9wq8"> 184 </td>
    <td class="tg-9wq8"> 1,174 </td>
    <td class="tg-9wq8"> 310 </td>
    <td class="tg-9wq8"> 1,182 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 1,166 </td>
    <td class="tg-9wq8"> 8,965 </td>
    <td class="tg-9wq8"> 95 </td>
    <td class="tg-9wq8"> 443 </td>
    <td class="tg-9wq8"> 138 </td>
    <td class="tg-9wq8"> 442 </td>
  </tr>
  <tr>
    <td class="tg-lboi">Hotel</td>
    <td class="tg-9wq8"> 1,599 </td>
    <td class="tg-9wq8"> 9,721 </td>
    <td class="tg-9wq8"> 997 </td>
    <td class="tg-9wq8"> 6,234 </td>
    <td class="tg-9wq8"> 277 </td>
    <td class="tg-9wq8"> 1,841 </td>
    <td class="tg-9wq8"> 325 </td>
    <td class="tg-9wq8"> 1,646 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 978 </td>
    <td class="tg-9wq8"> 6,737 </td>
    <td class="tg-9wq8"> 205 </td>
    <td class="tg-9wq8"> 1,422 </td>
    <td class="tg-9wq8"> 316 </td>
    <td class="tg-9wq8"> 1,769 </td>
  </tr>
  <tr>
    <td class="tg-d459">SportsEvent</td>
    <td class="tg-9wq8"> 833 </td>
    <td class="tg-9wq8"> 3,301 </td>
    <td class="tg-9wq8"> 572 </td>
    <td class="tg-9wq8"> 2,536 </td>
    <td class="tg-9wq8"> 67 </td>
    <td class="tg-9wq8"> 289 </td>
    <td class="tg-9wq8"> 194 </td>
    <td class="tg-9wq8"> 476 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 1,126 </td>
    <td class="tg-9wq8"> 4,669 </td>
    <td class="tg-9wq8"> 141 </td>
    <td class="tg-9wq8"> 435 </td>
    <td class="tg-9wq8"> 140 </td>
    <td class="tg-9wq8"> 452 </td>
  </tr>
  <tr>
    <td class="tg-lboi">Restaurant</td>
    <td class="tg-9wq8"> 1,176 </td>
    <td class="tg-9wq8"> 9,036 </td>
    <td class="tg-9wq8"> 948 </td>
    <td class="tg-9wq8"> 8,162 </td>
    <td class="tg-9wq8"> 33 </td>
    <td class="tg-9wq8"> 210 </td>
    <td class="tg-9wq8"> 195 </td>
    <td class="tg-9wq8"> 664 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 1,182 </td>
    <td class="tg-9wq8"> 8,556 </td>
    <td class="tg-9wq8"> 111 </td>
    <td class="tg-9wq8"> 768 </td>
    <td class="tg-9wq8"> 215 </td>
    <td class="tg-9wq8"> 996 </td>
  </tr>
  <tr>
    <td class="tg-d459">Book</td>
    <td class="tg-9wq8"> 2,544 </td>
    <td class="tg-9wq8"> 10,807 </td>
    <td class="tg-9wq8"> 1,720 </td>
    <td class="tg-9wq8"> 7,633 </td>
    <td class="tg-9wq8"> 347 </td>
    <td class="tg-9wq8"> 1,447 </td>
    <td class="tg-9wq8"> 477 </td>
    <td class="tg-9wq8"> 1,727 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 1,124 </td>
    <td class="tg-9wq8"> 6,674 </td>
    <td class="tg-9wq8"> 206 </td>
    <td class="tg-9wq8"> 1,332 </td>
    <td class="tg-9wq8"> 353 </td>
    <td class="tg-9wq8"> 1,387 </td>
  </tr>
  <tr>
    <td class="tg-lboi">TVEpisode</td>
    <td class="tg-9wq8"> 418 </td>
    <td class="tg-9wq8"> 1,197 </td>
    <td class="tg-9wq8"> 231 </td>
    <td class="tg-9wq8"> 832 </td>
    <td class="tg-9wq8"> 18 </td>
    <td class="tg-9wq8"> 56 </td>
    <td class="tg-9wq8"> 169 </td>
    <td class="tg-9wq8"> 309 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 370 </td>
    <td class="tg-9wq8"> 1,600 </td>
    <td class="tg-9wq8"> 46 </td>
    <td class="tg-9wq8"> 110 </td>
    <td class="tg-9wq8"> 48 </td>
    <td class="tg-9wq8"> 110 </td>
  </tr>
  <tr>
    <td class="tg-d459">JobPosting</td>
    <td class="tg-9wq8"> 147 </td>
    <td class="tg-9wq8"> 738 </td>
    <td class="tg-9wq8"> 110 </td>
    <td class="tg-9wq8"> 586 </td>
    <td class="tg-9wq8"> 12 </td>
    <td class="tg-9wq8"> 74 </td>
    <td class="tg-9wq8"> 25 </td>
    <td class="tg-9wq8"> 78 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 2,135 </td>
    <td class="tg-9wq8"> 8,900 </td>
    <td class="tg-9wq8"> 240 </td>
    <td class="tg-9wq8"> 1,163 </td>
    <td class="tg-9wq8"> 484 </td>
    <td class="tg-9wq8"> 1,170 </td>
  </tr>
  <tr>
    <td class="tg-lboi">Museum</td>
    <td class="tg-9wq8"> 53 </td>
    <td class="tg-9wq8"> 386 </td>
    <td class="tg-9wq8"> 49 </td>
    <td class="tg-9wq8"> 368 </td>
    <td class="tg-9wq8"> - </td>
    <td class="tg-9wq8"> - </td>
    <td class="tg-9wq8"> 4 </td>
    <td class="tg-9wq8"> 18 </td>

    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> </td>
    <td class="tg-9wq8"> 145 </td>
    <td class="tg-9wq8"> 513 </td>
    <td class="tg-9wq8"> 18 </td>
    <td class="tg-9wq8"> 63 </td>
    <td class="tg-9wq8"> 18 </td>
    <td class="tg-9wq8"> 73 </td>
  </tr>
</tbody>
</table></div>

<p>
  The CTA datasets include 94 different labels where 4 are new types and 17 are class+name. The CPA datasets include 181 different labels. In <i>Table 2</i> the top 5 
  CPA and CTA labels with the most columns in the sets are shown.
</p>

<p>

  (TABLE)
  (+++ How to find annotations in files)

</p>

<span id="toc6"></span>
<h2>3. Challenges Subsets</h2>

<p>
  Alongside the training, validation and testing sets for both table annotation problems, some subsets of the testing set are made available. These include the missing values subset,
  the corner cases subset, the value heterogeneity subset and the random subset. Each corresponds to the four selection types of columns that were used to form the full testing set.
  The purpose of these subsets would be to evaluate separately how a model performs on different types of challenging columns and on the randomly selected columns.
        
</p>

<p>
  <b>(i) Missing values:</b> This subset includes columns that have a density between 10 and 70 percent, i.e. 10-70 percent of their rows are filled with values. For the CPA problem 3,572
  column pairs annotated with 125 CPA labels and for CTA 3,073 columns annotated with 64 CTA labels.
</p>

<p>
  <b>(ii) Value heterogeneity:</b> This subset includes columns that have the same label but their values are expressed in different formats. The CPA columns that are selected for this
  challenge are the different date columns, duration, height, weight, width, price, telephone, faxNumber, servingSize, size and salary. There are in total 2,141 columns for CPA in this
  subset annotated with 30 different CPA labels.
  For CTA, the selected columns are duration, height, weight, width, Date, DateTime, Time, price, priceRange, telephone and faxNumber. The CTA columns are in total 958 and are annotated with
  10 CTA labels.
</p>

<p>
  <b>(iii) Corner Cases:</b> This subset includes columns that are considered to be corner cases. For the CPA and CTA problems corner cases are columns that are annotated with the same
  semantic label but their values are dissimilar or columns that are annotated with different semantic labels but their values are very similar. There are 3,510 corner cases included in the
  CPA corner cases subset which are annotated with 106 different CPA labels, while in the CTA subset there are 5,142 corner case columns annotated with 70 different CTA labels.
</p>

<p>
  <b>(iv) Random Selection: </b> This subset includes randomly selected columns for each label of each task. In total there are 12,664 randomly selected columns for CTA and 15,758 randomly
  selected columns in the CPA subset.
</p>

<p>
  In each subset and for each label included there is a minimum of 10 examples. A summary of all statistics for each subset is shown in <i>Table 3</i>.
</p>

<p>(TABLE)</p>


<span id="toc6"></span>
<h2>4. Baselines</h2>

<p>
  To evaluate the created sets some baseline experiments were ran. The first baseline method is a non-deep learning method that uses TF-IDF to create features based on the column values
  and uses a Random Forest classifier to make predictions. The second baseline is a deep learning method called TURL [ref] which uses a Transformer-based architecture and is 
  pre-trained on relational tables using TinyBERT. For the experiments the micro-F1 score is used to report the results of each baseline method. The results are summarized in <i>Table 4</i>.
</p>


<p>
  <table id="tg-BNPY5" class="tg">
    <caption>Table 4. Baseline Results on Random Forest and TURL for different splits</caption>
    <thead>
      <tr>
            <th></th>

        <th class="tg-mkpc" colspan="5">Column Type Annotation</th>
        <th class="tg-ixdq" colspan="5">Column Pair Annotation</th>
      </tr>
    </thead>
    <tbody>
      <tr>
            <td>Method</td>
            <td>Missing Values</td>
            <td>Value Heterogeneity</td>
            <td>Corner Cases</td>
            <td>Random Selection</td>
            <td>Full Set</td>

            <td>Missing Values</td>
            <td>Value Heterogeneity</td>
            <td>Corner Cases</td>
            <td>Random Selection</td>
            <td>Full Set</td>

      </tr>
      <tr>
        <td class="tg-d459">Random Forest <br/> TF-IDF </td>
        <td>0.65</td>
        <td>0.63</td>
        <td>0.56</td>
        <td>0.58</td>
        <td>0.58</td>

        <td>0.46</td>
        <td>0.58</td>
        <td>0.45</td>
        <td>0.47</td>
        <td>0.47</td>
      </tr>
      <tr>
        <td class="tg-lboi">TURL</td>
        <td>0.64</td>
        <td>0.81</td>
        <td>0.64</td>
        <td>0.72</td>
        <td>0.76</td>

        <td>0.64</td>
        <td>0.74</td>
        <td>0.58</td>
        <td>0.68</td>
        <td>0.69</td>
      </tr>
    </tbody>
  </table>
</p>

<p>
  The top 5 labels that reached the highest score and the top 5 labels that reached the lowest score for both CPA and CTA tasks are listed in <i>Table 5</i>.
</p>


<p>
  <table>
  <caption>Table 5. Top 5 best f1 scores for labels in CTA and CPA</caption>
  <thead>
    <tr>
      <th></th>
      <th class="tg-mkpc">CPA</th>
      <th class="tg-mkpc"></th>
      <th class="tg-mkpc">CTA</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Label</td>
      <td>Overall F1</td>
      <td>Label</td>
      <td>Overall F1</td>
    </tr>

    <tr>
      <td colspan="4">Top 5 Best scores</td>
    </tr>

    <tr>
      <td>eventAttendanceMode</td>
      <td>100</td>
      <td>EventAttendanceModeEnumeration</td>     
      <td>97.49</td>     
    </tr>

    <tr>
      <td>makesOffer</td>
      <td>98</td>
      <td>ItemAvailability</td>     
      <td>97</td>
    </tr>
        
    <tr>
      <td>eventStatus</td>
      <td>96</td>
      <td>OfferItemCondition</td>     
      <td>96.76</td>
    </tr>

    <tr>
      <td>hasMenu</td>
      <td>96</td>
      <td>WarrantyPromise</td>     
      <td>96.40</td>
    </tr>

    <tr>
      <td>knowsLanguage</td>
      <td>95</td>
      <td>Action</td>     
      <td>96.36</td>
    </tr>

    <tr>
      <td colspan="4">Top 5 Worst scores</td>
    </tr>

    <tr>
      <td>seasonNumber</td>
      <td>0</td>
      <td>identifierNameAP</td>     
      <td>26.61</td>
    </tr>

    <tr>
      <td>countryOfOrigin</td>
      <td>0</td>
      <td>AggregateRating</td>     
      <td>26.19</td>
    </tr>
      
    <tr>
      <td>exampleOfWork</td>
      <td>0</td>
      <td>CreativeWork/name</td>     
      <td>23.53</td>
    </tr>

    <tr>
      <td>alternateName</td>
      <td>0</td>
      <td>CreativeWorkSeries</td>     
      <td>19.51</td>
    </tr>

    <tr>
      <td>datePosted</td>
      <td>0</td>
      <td>MusicGroup</td>     
      <td>17.65</td>
    </tr>

  </tbody>              
  </table>   
</p>


<span id="toc3"></span>
<h2>5. Download</h2>
			
        <p>The code used for the creation of the corpus will be put in github</a>.</p>
			<p>

				<span id="toc4"></span>
				<h2>			    6. Feedback</h2>
				<p>Please send questions and feedback to the <a
						href="http://groups.google.com/group/web-data-commons">Web Data
						Commons Google Group</a>.<br /><br />
					More information about the <strong>Web Data Commons</strong> project is found <a href="http://webdatacommons.org/">here</a>.</p>

				<span id="toc5"></span>
				<h2>7. Other Table Corpora</h2>
				<p>Links to other publicly accessible table corpora are found below:</p>
				
				<p>&nbsp;</p>

				<script type="text/javascript">
					$('#toc').toc({
						'selectors': 'h2', //elements to use as headings
						'container': '#toccontent', //element to find all selectors in
						'smoothScrolling': true, //enable or disable smooth scrolling on click
						'prefix': 'toc', //prefix for anchor tags and class names
						'highlightOnScroll': true, //add class to heading that is currently in focus
						'highlightOffset': 100, //offset to trigger the next headline
						'anchorName': function (i, heading, prefix) { //custom function for anchor name
							return prefix + i;
						}
					});
					$('[id*="link_"]').each(function () {
						var element = $(this);
						element.click(function (e) {
							e.preventDefault();
							var id = element.attr("id").split("_")[1];
							element.parent().removeClass("show").addClass("no-show");
							$('#charts_' + id).removeClass("no-show").addClass("show");
						});
					});
					$('[id*="colapse_"]').each(function () {
						var element = $(this);
						element.click(function (e) {
							e.preventDefault();
							var id = element.attr("id").split("_")[1];
							element.parent().removeClass("show").addClass("no-show");
							$('#intro_' + id).removeClass("no-show").addClass("show");
						});
					});
					document.getElementById("defaultOpen").click();
				</script>
</body>

</html>
