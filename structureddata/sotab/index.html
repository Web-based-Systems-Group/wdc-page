<!DOCTYPE html>
<html>

<head>
	<title>The WDC Schema.org Table Annotation Benchmark (SOTAB)</title>
	<link rel='stylesheet' href='http://webdatacommons.org/style.css' type='text/css' media='screen' />
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<style>
		.tar {
			text-align: right;
		}

		.rtable {
			float: right;
			padding-left: 10px;
		}

		.smalltable,
		.smalltable TD,
		.smalltable TH {
			font-size: 9pt;
		}

		.tab {
			overflow: hidden;
			border: 1px solid #ccc;
			background-color: #eaf3fa;
			clear: both;
			padding-left: 25px;
			width: 650px;
		}

		.tab button {
			background-color: inherit;
			float: left;
			border: none;
			outline: none;
			cursor: pointer;
			padding: 15px 60px;
			transition: 0.3s;
		}

		.tab button:hover {
			background-color: #ddd;
		}

		.tab button.active {
			background-color: #ccc;
		}

		.tabcontent {
			display: none;
			padding: 6px 12px;
			border-top: none;
			animation: fadeEffect 1s;
			width: 500px
		}

		.table-wrapper {
			position: relative;
		}

		.table-scroll {
			height: 240px;
			overflow: auto;
			margin-top: -10px;
		}

		.show {
			display: block;
		}

		.no-show {
			display: none;
		}

		caption {
			caption-side: top;
			font-style: italic;
		}

		td[scope="mergedcol"] {
			text-align: center;
		}

		tr.bordered {
			border-bottom: 1px solid #000;
		}

		hr {
			width: 50%;
			margin: 20px 0;
			/* This leaves 10px margin on left and right. If only right margin is needed try margin-right: 10px; */
		}

    .tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}
    .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
    .tg .tg-ixdq{border-color:inherit;font-weight:bold;position:-webkit-sticky;position:sticky;text-align:center;top:-1px;
      vertical-align:middle;will-change:transform}
    .tg .tg-mkpc{border-color:inherit;font-weight:bold;position:-webkit-sticky;position:sticky;text-align:left;top:-1px;
      vertical-align:middle;will-change:transform}
    .tg .tg-d459{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-kyy7{background-color:#f9f9f9;border-color:inherit;text-align:center;vertical-align:middle}
    .tg .tg-h2b0{background-color:#FFF;border-color:inherit;color:#333;text-align:center;vertical-align:middle}
    .tg-sort-header::-moz-selection{background:0 0}
    .tg-sort-header::selection{background:0 0}.tg-sort-header{cursor:pointer}
    .tg-sort-header:after{content:'';float:right;margin-top:7px;border-width:0 5px 5px;border-style:solid;
      border-color:#404040 transparent;visibility:hidden}
    .tg-sort-header:hover:after{visibility:visible}
    .tg-sort-asc:after,.tg-sort-asc:hover:after,.tg-sort-desc:after{visibility:visible;opacity:.4}
    .tg-sort-desc:after{border-bottom:none;border-width:5px 5px 0}@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}

    .tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}
    .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
    .tg .tg-baqh{text-align:center;vertical-align:top}
    .tg .tg-zyik{border-color:inherit;font-weight:bold;position:-webkit-sticky;position:sticky;text-align:center;top:-1px;
      vertical-align:top;will-change:transform}
    .tg .tg-ixdq{border-color:inherit;font-weight:bold;position:-webkit-sticky;position:sticky;text-align:center;top:-1px;
      vertical-align:middle;will-change:transform}
    .tg .tg-yy5h{background-color:#F0F0F0;border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-o939{background-color:#f0f0f0;border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-ufyq{background-color:#f0f0f0;font-weight:bold;text-align:center;vertical-align:top}
    .tg .tg-asv9{background-color:#F0F0F0;border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}
    .tg .tg-d459{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-dzk6{background-color:#f9f9f9;text-align:center;vertical-align:top}
    .tg .tg-kyy7{background-color:#f9f9f9;border-color:inherit;text-align:center;vertical-align:middle}
    .tg-sort-header::-moz-selection{background:0 0}
    .tg-sort-header::selection{background:0 0}.tg-sort-header{cursor:pointer}
    .tg-sort-header:after{content:'';float:right;margin-top:7px;border-width:0 5px 5px;border-style:solid;
      border-color:#404040 transparent;visibility:hidden}
    .tg-sort-header:hover:after{visibility:visible}
    .tg-sort-asc:after,.tg-sort-asc:hover:after,.tg-sort-desc:after{visibility:visible;opacity:.4}
    .tg-sort-desc:after{border-bottom:none;border-width:5px 5px 0}@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}

		.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;display:inline-block;margin-right:50px;}
    .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
      font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-pl4i{background-color:#f0f0f0;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-j1i3{border-color:inherit;position:-webkit-sticky;position:sticky;text-align:left;top:-1px;vertical-align:top;
      will-change:transform}
    .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
    .tg .tg-ixdq{border-color:inherit;font-weight:bold;position:-webkit-sticky;position:sticky;text-align:center;top:-1px;
      vertical-align:middle;will-change:transform}
    .tg .tg-yy5h{background-color:#F0F0F0;border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-o939{background-color:#f0f0f0;border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-45e1{background-color:#f0f0f0;border-color:inherit;font-weight:bold;text-align:left;vertical-align:middle}
    .tg .tg-nrix{text-align:center;vertical-align:middle}
    .tg .tg-d459{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:middle}
    .tg .tg-kyy7{background-color:#f9f9f9;border-color:inherit;text-align:center;vertical-align:middle}
    .tg .tg-57iy{background-color:#f9f9f9;text-align:center;vertical-align:middle}
    .tg-sort-header::-moz-selection{background:0 0}
    .tg-sort-header::selection{background:0 0}.tg-sort-header{cursor:pointer}
    .tg-sort-header:after{content:'';float:right;margin-top:7px;border-width:0 5px 5px;border-style:solid;
      border-color:#404040 transparent;visibility:hidden}
    .tg-sort-header:hover:after{visibility:visible}
    .tg-sort-asc:after,.tg-sort-asc:hover:after,.tg-sort-desc:after{visibility:visible;opacity:.4}
    .tg-sort-desc:after{border-bottom:none;border-width:5px 5px 0}@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}
				
		@keyframes fadeEffect {
			from {
				opacity: 0;
			}

			to {
				opacity: 1;
			}
		}
	</style>
	
	<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
	<script type="text/javascript" src="../../jquery.toc.min.js"></script>
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-30248817-1']);
		_gaq.push(['_trackPageview']);

		(function () {
			var ga = document.createElement('script');
			ga.type = 'text/javascript';
			ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0];
			s.parentNode.insertBefore(ga, s);
		})();
	</script>
	<script type="application/ld+json">
		{
			"@context": "http://schema.org/",
			"@type": "Dataset",
			"name": "Web Data Commons - Schema.org Table Corpus",
			"description": "The product dataset consists of 20 million pairs of product offers referring to the same products. The offers were extracted from 43 thousand e-shops which provide schema.org annotations including some form of product ID such as a GTIN or MPN. We also created a gold standard by manually verifying 4400 pairs of offers belonging to four different product categories.",
			"url": "http://webdatacommons.org/structureddata/schemaorgtables/index.html",
			"keywords": [
				"table corpus",
				"product corpus",
				"large scale",
				"product matching",
				"entity matching"
			],
			"creator": [
				{
					"@type": "Person",
					"url": "https://www.uni-mannheim.de/dws/people/researchers/phd-students/ralph-peeters/"
					"name": "Ralph Peeters"
				},
				{
					"@type": "Person",
					"url": "https://www.uni-mannheim.de/dws/people/professors/prof-dr-christian-bizer/",
					"name": "Christian Bizer"
				}
			],
			"distribution": [{
				"@type": "DataDownload",
				"fileFormat": [
					"json"
				],
				//TODO: Change once final
				"contentUrl": "http://webdatacommons.org/structureddata/sotab/index.html"
			}],
			"citation": [

			]
		}
	</script>

<script charset="utf-8">var TGSort=window.TGSort||function(n){"use strict";function r(n){return n?n.length:0}function t(n,t,e,o=0){for(e=r(n);o<e;++o)t(n[o],o)}function e(n){return n.split("").reverse().join("")}function o(n){var e=n[0];return t(n,function(n){for(;!n.startsWith(e);)e=e.substring(0,r(e)-1)}),r(e)}function u(n,r,e=[]){return t(n,function(n){r(n)&&e.push(n)}),e}var a=parseFloat;function i(n,r){return function(t){var e="";return t.replace(n,function(n,t,o){return e=t.replace(r,"")+"."+(o||"").substring(1)}),a(e)}}var s=i(/^(?:\s*)([+-]?(?:\d+)(?:,\d{3})*)(\.\d*)?$/g,/,/g),c=i(/^(?:\s*)([+-]?(?:\d+)(?:\.\d{3})*)(,\d*)?$/g,/\./g);function f(n){var t=a(n);return!isNaN(t)&&r(""+t)+1>=r(n)?t:NaN}function d(n){var e=[],o=n;return t([f,s,c],function(u){var a=[],i=[];t(n,function(n,r){r=u(n),a.push(r),r||i.push(n)}),r(i)<r(o)&&(o=i,e=a)}),r(u(o,function(n){return n==o[0]}))==r(o)?e:[]}function v(n){if("TABLE"==n.nodeName){for(var a=function(r){var e,o,u=[],a=[];return function n(r,e){e(r),t(r.childNodes,function(r){n(r,e)})}(n,function(n){"TR"==(o=n.nodeName)?(e=[],u.push(e),a.push(n)):"TD"!=o&&"TH"!=o||e.push(n)}),[u,a]}(),i=a[0],s=a[1],c=r(i),f=c>1&&r(i[0])<r(i[1])?1:0,v=f+1,p=i[f],h=r(p),l=[],g=[],N=[],m=v;m<c;++m){for(var T=0;T<h;++T){r(g)<h&&g.push([]);var C=i[m][T],L=C.textContent||C.innerText||"";g[T].push(L.trim())}N.push(m-v)}t(p,function(n,t){l[t]=0;var a=n.classList;a.add("tg-sort-header"),n.addEventListener("click",function(){var n=l[t];!function(){for(var n=0;n<h;++n){var r=p[n].classList;r.remove("tg-sort-asc"),r.remove("tg-sort-desc"),l[n]=0}}(),(n=1==n?-1:+!n)&&a.add(n>0?"tg-sort-asc":"tg-sort-desc"),l[t]=n;var i,f=g[t],m=function(r,t){return n*f[r].localeCompare(f[t])||n*(r-t)},T=function(n){var t=d(n);if(!r(t)){var u=o(n),a=o(n.map(e));t=d(n.map(function(n){return n.substring(u,r(n)-a)}))}return t}(f);(r(T)||r(T=r(u(i=f.map(Date.parse),isNaN))?[]:i))&&(m=function(r,t){var e=T[r],o=T[t],u=isNaN(e),a=isNaN(o);return u&&a?0:u?-n:a?n:e>o?n:e<o?-n:n*(r-t)});var C,L=N.slice();L.sort(m);for(var E=v;E<c;++E)(C=s[E].parentNode).removeChild(s[E]);for(E=v;E<c;++E)C.appendChild(s[v+L[E-v]])})})}}n.addEventListener("DOMContentLoaded",function(){for(var t=n.getElementsByClassName("tg"),e=0;e<r(t);++e)try{v(t[e])}catch(n){}})}(document)</script>
				

</head>

<body>
	<div id="logo" style="text-align:right; background-color: white;">&nbsp;&nbsp;<a
			href="http://dws.informatik.uni-mannheim.de"><img src="../../images/ma-logo.gif"
				alt="University of Mannheim - Logo"></a></div>
	<div id="header">
		<h1 style="font-size: 250%;"> Web Data Commons - Schema.org Table Annotation Benchmark </h1>
	</div>
	<div id="authors">
    <a href="https://www.uni-mannheim.de/dws/people/researchers/phd-students/keti-korini/">Keti Korini</a><br>
		<a href="https://www.uni-mannheim.de/dws/people/researchers/phd-students/ralph-peeters/">Ralph Peeters</a><br>
		<a href="https://www.uni-mannheim.de/dws/people/professors/prof-dr-christian-bizer/">Christian Bizer</a><br />
	</div>
	<div id="content">
		<p>
			This page provides the <b>WDC Schema.org <a href="https://paperswithcode.com/task/table-annotation">Table Annotation</a> Benchmark</b> (SOTAB) for public download. SOTAB features two annotation tasks: Column Type Annotation and 
      Columns Property Annotation. The goal of the <a href="https://paperswithcode.com/task/column-type-annotation">Column Type Annotation (CTA)</a> task is to annotate the columns of a table with 93 
      <a href="https://schema.org/">Schema.org</a> types, such as telephone, duration, Place, or Organization. The goal of the <a href="https://paperswithcode.com/task/columns-property-annotation">Columns Property 
      Annotation (CPA)</a> task is to annotate pairs of table columns with one out of 180 <a href="https://schema.org/">Schema.org</a> properties, such as gtin13, startDate, priceValidUntil, or recipeIngredient. 
			The benchmark consists of <b>42,784 tables</b> annotated for CTA and <b>45,664 tables</b> annotated for CPA. The tables are split into training-, validation- and test sets for both tasks. 
      The tables cover 17 popular Schema.org types including Product, LocalBusiness, Event, and JobPosting. The tables originate from the <a href="http://webdatacommons.org/structureddata/schemaorgtables/">Schema.org Table Corpus</a>.
	  </p>
<h2 id="news">News</h2>
		<ul>
			<li><strong>2022-08:</strong> Initial version of the WDC Schema.org Table Annotation Benchmark (SOTAB) released.</li>
		</ul>
		<h2>Contents</h2>
		<ul>
			<li class="toc-h2 toc-active">
				<a href="#toc1"> 1. Introduction</a>
			</li>
			<li class="toc-h2 toc-active">
				<a href="#toc2"> 2. Schema.org Table Corpus</a>
			</li>

      <li class="toc-h2 toc-active">
				<a href="#toc3"> 3. Table Selection</a>
			</li>

      <li class="toc-h2 toc-active">
				<a href="#toc4"> 4. Label Sets for the CTA and CPA Task </a>
			</li>

      <li class="toc-h2 toc-active">
				<a href="#toc5"> 5. Dataset Profiling </a>
			</li>

      <li class="toc-h2 toc-active">
				<a href="#toc6"> 6. Testsets for Specific Challenges </a>
			</li>

      <li class="toc-h2 toc-active">
				<a href="#toc7"> 7. Baselines </a>
			</li>
			
			<li class="toc-h2 toc-active">
				<a href="#toc8"> 8. Download </a>
			</li>
			<li class="toc-h2 toc-active">
				<a href="#toc9"> 9. Other Table Annotation Benchmarks  </a>
			<li class="toc-h2 toc-active">
        <a href="#toc10"> 10. References </a>
			</li>
		</ul>

		



<span id="toc1"></span>
<h2>1. Introduction</h2>
<p>
  Understanding the semantics of table elements is a pre-requisite for many data integration tasks such as schema mapping or knowledge base augmentation.
  <a href="https://paperswithcode.com/task/table-annotation">Table annotation</a> is the task of annotating a table with terms/concepts from knowledge graph, database schema or vocabulary. 
  Table annotation consists of five different tasks: column type annotation (CTA), columns property annotation (CPA), cell entity annotation (CEA), row annotation, and table type detection. 
  Table annotation has attracted <a href="https://paperswithcode.com/task/table-annotation">quite some attention in the research community</a> in recent years and there are active benchmarking campaigns on table annotation, such as <a href="https://www.cs.ox.ac.uk/isg/challenges/sem-tab/">SemTab</a>.
  <span id="toc2"></span>
</p>

<p>
  CTA is the task of annotating table columns with the type of entities described in the column, while CPA aims to annotate the relationship between the main column of a table (usually the leftmost) 
  with another column in the table.
  An example of CTA and CPA annotations is given in <i>Figure 1</i>. For the CTA task, the first column is annotated with the label <b>"Hotel/name"</b> as the column contains the names of hotels. Further CTA annotations are <b>"addressLocation", "addressLocality", "Country" and "currency"</b> For the CPA task
  the relationship between the main column <b>"Hotel/name"</b> and the second column is annotated using <b>"streetAddress"</b> as the second column shows the address of the hotels in the first column.
</p>

<figure>
  <img src="images/table_CTA_CPA.jpg" alt="Table Annotation Example">
  <figcaption><b><a id="Fig1"></a>Figure 1:</b> Example of table annotation. The CTA labels are placed on the top of the columns, while the CPA labels are placed between the main column (leftmost column) and another column.</figcaption>
</figure>


<p>
  In order to compare the performance of table annotation methods, benchmark datasets covering a wide range of topics are needed. The aim of this work is to create 
  and make available datasets for benchmarking two of the table annotation tasks: column type annotation (CTA) and column pair annotation (CPA). 
  The datasets are build by using tables from the <a href="http://webdatacommons.org/structureddata/schemaorgtables/">Schema.org Table Corpus</a>.
</p>

<span id="toc2"></span>
<h2>2. Schema.org Table Corpus </h2>
<p>
        The original <a href="http://webdatacommons.org/structureddata/schemaorgtables/">Schema.org Table Corpus</a> used for the creation of this benchmark was created using schema.org data from the December 2020 version of the <a href="http://webdatacommons.org/structureddata/2020-12/stats/schema_org_subsets.html">Web Data Commons Microdata and JSON-LD corpus</a>. The Schema.org Table Corpus consists of 4.2 million relational tables covering 43 schema.org classes.
Each table in the Schema.org Table Corpus contains the descriptions of all entities of a specific Schema.org class that were extracted for a specific host, e.g. movie records from imdb.com or product records from ebay.com or rakuten.co.jp. 
All extracted entities of one Schema.org class are collected per host and subsequently passed through a pipeline consisting of three steps, (i) Attribute extraction, (ii) Removal of listing pages and sparse entities, and (iii) content-based deduplication. 
Following the cleansing steps, only attributes with a density of at least 25% are retained per table. For more information about these steps and the general Schema.org Table Corpus creation process, statistics files and download links, please visit the <a href="http://webdatacommons.org/structureddata/schemaorgtables/">Schema.org Table Corpus website</a>. 
  
</p>

<span id="toc3"></span>
<h2> 3. Table Selection </h2>
<p>
  For the creation of the datasets, the top100 and minimum3 sets of tables from the Schema.org Table Corpus were considered and tables belonging to the 17 classes with the most columns and tables. 
  Three steps were taken to select tables for CTA and CPA tables in SOTAB: 
  (i) Language Detection, (ii) Filtering based on number of columns, (iii) Finding challenging columns. These steps are explained more in detail below.
</p>

<p>
  <b>(i) Language Detection: </b> The purpose of this step was to filter out tables and rows that were not in the English language. 
  The <a href="https://fasttext.cc/docs/en/language-identification.html">FastText</a> language detection model was used for detecting the language of rows/tables. The <i>description</i> column
  and <i>disambiguatingDescription</i> column were chosen to check the language of a table, as they are columns that contain longer textual values that can indicate the language used. If a table did
  not have one of these columns the concatenation of all other columns was used to determine the language. Tables and rows where the model was at least 50% confident that the language is English and 
  where the model is not at least 50% confident that the language used is not English are kept and the rest are discarded.
</p>

<p>
  <b>(ii) Filtering out tables based on number of columns: </b> In the second step, tables that had less than 10 rows and less than 3 columns were filtered out. Furthermore, as the column headers belong to
  Schema.org properties that can be used for annotating the selected tables in later steps, to insure that enough columns are available for each Schema.org property, columns that belonged to properties with 
  less than 50 columns in total across each table type were filtered out.
</p>

<p>
  <b>(iii) Finding challenging columns: </b> In the selection phase of the tables, random columns and challenging columns were chosen for each 
  CTA and CPA label. These challenging columns belong to three challenges: (a) Missing Values, (b) Format Heterogeneity and (c) Corner Cases.
  In the first challenge, columns that contained missing values were selected, in the second one columns whose values were described using 
  different formats, like different date formats or different measurement metrics were selected and finally in the third challenge columns 
  that have the same label but dissimilar values and columns that have different labels but have similar values are included. These similar and dissimilar columns
  were detected using a TF-IDF and cosine similarity method for textual columns and a Kolmogorov–Smirnov statistical test for numerical and DateTime columns.
  <br/><br/>

  Some examples of columns selected for the three challenges are shown in <i>Table 1</i>.
</p>

<table>
  <caption>Table 1. Challenge columns examples.</caption>
  <tbody>
    <tr>
      <th></th>
      <th><b>Label</b></th>
      <th><b>Column Values</b></th>
    </tr>
    <tr>
      <td rowspan="2"> <i>(a) Missing Values</i> </td>
      <td> <b>bestRating</b>  </td>
      <td> 5.0, 5.0, None, None ... </td>
    </tr>
    <tr>
      <td> <b>priceCurrency</b> </td>
      <td> 'GBP', 'GBP', None, None ... </td>
    </tr>
    <tr>
      <td rowspan="2"> <i>(b) Format Heterogeneity</i> </td>
      <td> <b>duration</b> </td>
      <td> 'Runtime: 86 mins', 'Runtime: 76 mins' ... </td>
    </tr>
    <tr>
      <td> <b>duration</b> </td>
      <td> '8:11', '4:14', '0:48', '5:51', '4:55' ... </td>
    </tr>
    <tr>
      <td rowspan="2"> <i>(c) Corner Cases</i> </td>
      <td> <b>startDate</b>  </td>
      <td>'2020-09-12T00:00:00.000Z', '2020-05-22T01:00:00.000Z', '2020-08-07T00:00:00.000Z' ... </td>
    </tr>
    <tr>
      <td> <b>endDate</b> </td>
      <td>'2020-09-20T14:00:00.000Z', '2020-05-24T10:00:00.000Z', '2020-08-09T15:00:00.000Z' ... </td>
    </tr>
  </tbody>
</table>



<span id="toc4"></span>
<h2> 4. Label Sets for the CTA and CPA Task</h2>

<p>
  To annotate the columns from the final selected tables for CTA and CPA Schema.org vocabulary was used. Column pairs in the CPA problem were annotated using Schema.org properties.
  As the Schema.org Table Corpus tables have Schema.org properties as column headers this is done directly by matching a column header to
  a Schema.org property. In the CTA task columns were annotated using Schema.org types. These types were derived from the expected types of a Schema.org property 
  of a specific column. As some Schema.org properties can have multiple Schema.org types, a manual selection was done to choose the most 
  general type for annotation. In some cases, we keep as CTA labels Schema.org properties with the purpose of including more fine-grained 
  labels. An example is the column named pricecurrency, where its corresponding Schema.org property <i>priceCurrency</i> is expected to 
  be of type <i>Text</i>. In this case we annotate the column pricecurrency for the CTA task as <i>currency</i>, which describes the semantics 
  of this column better than the more general type <i>Text</i>. In other cases additionally, we create some labels with the same purpose. 
  An example is for the column named isbn, its Schema.org property <i>isbn</i> is expected to be of type <i>Text</i>. We annotate this column 
  with a new label that we call <i>IdentifierAT</i> (AT standing for Additional Type) as we believe it is benefitial for systems to be able 
  to distinguish identifiers from simple text.
</p>

<p>
  The final label space for contains <b>93 CTA labels</b> and <b>180 CPA labels</b> covering more than 17 Schema.org types. 
  In <i>Table 2</i> some 5 column names and their CPA label and CTA label from the 3 largest table types of SOTAB are shown.
  For the full set of column names and their statistics, as well as the label space for CTA and CPA, a file is provided for download in the 
  <a href="#toc8">Download</a> section.
</p>

<p>
  <table>
    <caption> Table 2: Columns' CTA and CPA labels of 3 Schema.org table types</caption>
    <thead>
      <th class="tg-mkpc">Table Type</th>
      <th class="tg-mkpc">CPA label</th>
      <th class="tg-mkpc">CTA label</th>

      <th class="tg-mkpc">Table Type</th>
      <th class="tg-mkpc">CPA label</th>
      <th class="tg-mkpc">CTA label</th>

      <th class="tg-mkpc">Table Type</th>
      <th class="tg-mkpc">CPA label</th>
      <th class="tg-mkpc">CTA label</th>
    </thead>
    <tbody>
      <tr>
        <td rowspan="5"> <i>Product</i> </td>
        <td>offers</td>
        <td>Offer</td>

        <td rowspan="5"> <i>Event</i> </td>
        <td> location </td>
        <td> Place </td>

        <td rowspan="5"> <i>Recipe</i> </td>
        <td> cookTime </td>
        <td> Duration </td>

      </tr>
      <tr>
        <td> priceCurrency </td>
        <td> currency </td>

        <td> description </td>
        <td> Text </td>

        <td> recipeIngredient </td>
        <td> Text </td>

      </tr><tr>
        <td> price </td>
        <td> price </td>

        <td> eventStatus </td>
        <td> EventStatusType </td>

        <td> prepTime </td>
        <td> Duration </td>

      </tr><tr>
        <td> priceValidUntil </td>
        <td> Date </td>

        <td> duration </td>
        <td> Duration </td>

        <td> calories </td>
        <td> Energy </td>

      </tr><tr>
        <td> sku </td>
        <td> IdentifierAT </td>

        <td> email </td>
        <td> email </td>

        <td> fatContent </td>
        <td> Mass </td>

      </tr>
    </tbody>
  </table>
</p>


<span id="toc5"></span>
<h2> 5. Dataset Profiling</h2>
<p>
  The SOTAB consists of <b>174,708 columns</b> annotated from <b> 42,784 tables</b> for CTA and <b>168,254 column</b> pairs annotated from <b>45,664</b> tables for CPA, distributed across 17 Schema.org types, like 
  <i>Product</i>, <i>Event</i> and <i>Place</i>. The tables do not include any metadata, i.e. no table headers 
  and no table captions are available. The benchmark includes columns with textual, numerical and dateTime column values. The CPA and CTA tables are split with a 80:10:10 ration into fixed
  training, validation and testing splits. In addition, we offer a smaller subset of the training set to help with the comparison of how models perform with different numbers of examples.     
<em>Table 3</em> gives an overview of the number of tables and columns per type and their median columns for CTA and CPA and for each of their splits.
</p> 
			 
<div class="tg-wrap"><table id="tg-tKOYO" class="tg">
<caption>Table 3: Number of tables, rows and median rows per table for CPA and CTA tables</caption>
<thead>
  <tr>
    <th class="tg-j1i3"></th>
    <th class="tg-ixdq" colspan="11">Column Type Annotation</th>
    <th class="tg-ixdq" colspan="11">Column Pair Annotation</th>
  </tr>
</thead>
<tbody>
<tr>
    <th class="tg-j1i3"></th>
    <th class="tg-ixdq" colspan="3">Overall</th>
    <th class="tg-ixdq" colspan="2">Training Set</th>
    <th class="tg-ixdq" colspan="2">Small Training Set</th>
    <th class="tg-ixdq" colspan="2">Validation Set</th>
    <th class="tg-ixdq" colspan="2">Testing Set</th>

    <th class="tg-ixdq" colspan="3">Overall</th>
    <th class="tg-ixdq" colspan="2">Training Set</th>
    <th class="tg-ixdq" colspan="2">Small Training Set</th>
    <th class="tg-ixdq" colspan="2">Validation Set</th>
    <th class="tg-ixdq" colspan="2">Testing Set</th>
  </tr>
  <tr>
    <td class="tg-45e1">Schema.org Type</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939">median rows/cols</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>

    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939">median rows/cols</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
    <td class="tg-o939"># tables</td>
    <td class="tg-o939"># columns</td>
  </tr>
  <tr>
    <td class="tg-lboi">Overall</td>
    <td class="tg-9wq8"> 42,784 </td>
    <td class="tg-9wq8"> 174,708 </td>
    <td class="tg-9wq8"> 39 / 8 </td>
    <td class="tg-9wq8"> 32,409 </td>
    <td class="tg-9wq8"> 135,016 </td>
    <td class="tg-9wq8"> 7,996 </td>
    <td class="tg-9wq8"> 33,411 </td>
    <td class="tg-9wq8"> 4,211 </td>
    <td class="tg-9wq8"> 17,911 </td>
    <td class="tg-9wq8"> 6,164 </td>
    <td class="tg-9wq8"> 21,781 </td>

    <td class="tg-9wq8"> 168,254 </td>
    <td class="tg-9wq8"> 45,664 </td>
    <td class="tg-9wq8"> 45 / 8 </td>
    <td class="tg-9wq8"> 33,679 </td>
    <td class="tg-9wq8"> 126,733 </td>
    <td class="tg-9wq8"> 8,435 </td>
    <td class="tg-9wq8"> 31,878 </td>
    <td class="tg-9wq8"> 4,263 </td>
    <td class="tg-9wq8"> 16,637 </td>
    <td class="tg-9wq8"> 7,722 </td>
    <td class="tg-9wq8"> 24,884 </td>
  </tr>
  <tr>
    <td class="tg-lboi">Product</td>
    <td class="tg-9wq8"> 9,498 </td>
    <td class="tg-9wq8"> 25,079 </td>
    <td class="tg-9wq8"> 60 / 8 </td>
    <td class="tg-9wq8"> 7,527 </td>
    <td class="tg-9wq8"> 19,694 </td>
    <td class="tg-9wq8"> 1,862 </td>
    <td class="tg-9wq8"> 4,106 </td>
    <td class="tg-9wq8"> 833 </td>
    <td class="tg-9wq8"> 1,888 </td>
    <td class="tg-9wq8"> 1,138 </td>
    <td class="tg-9wq8"> 3,497 </td>

    <td class="tg-9wq8"> 11,664 </td>
    <td class="tg-9wq8"> 27,953 </td>
    <td class="tg-9wq8"> 66 / 10 </td>
    <td class="tg-9wq8"> 8,976 </td>
    <td class="tg-9wq8"> 20,996 </td>
    <td class="tg-9wq8"> 2,130 </td>
    <td class="tg-9wq8"> 4,877 </td>
    <td class="tg-9wq8"> 951 </td>
    <td class="tg-9wq8"> 2,322 </td>
    <td class="tg-9wq8"> 1,737 </td>
    <td class="tg-9wq8"> 4,635 </td>
  </tr>
  <tr>
    <td class="tg-d459">Event</td>
    <td class="tg-9wq8"> 6,016 </td>
    <td class="tg-9wq8"> 20,456 </td>
    <td class="tg-9wq8"> 23 / 8 </td>
    <td class="tg-9wq8"> 4,566 </td>
    <td class="tg-9wq8"> 15,444 </td>
    <td class="tg-9wq8"> 1,085 </td>
    <td class="tg-9wq8"> 4,551 </td>
    <td class="tg-9wq8"> 619 </td>
    <td class="tg-9wq8"> 2,349 </td>
    <td class="tg-9wq8"> 831 </td>
    <td class="tg-9wq8"> 2,663 </td>

    <td class="tg-9wq8"> 6,889 </td>
    <td class="tg-9wq8"> 21,271 </td>
    <td class="tg-9wq8"> 26 / 7 </td>
    <td class="tg-9wq8"> 5,139 </td>
    <td class="tg-9wq8"> 16,769 </td>
    <td class="tg-9wq8"> 1,404 </td>
    <td class="tg-9wq8"> 4,589 </td>
    <td class="tg-9wq8"> 735 </td>
    <td class="tg-9wq8"> 1,981 </td>
    <td class="tg-9wq8"> 1,015 </td>
    <td class="tg-9wq8"> 2,521 </td>
  </tr>
  <tr>
    <td class="tg-lboi">Recipe</td>
    <td class="tg-9wq8"> 3,272 </td>
    <td class="tg-9wq8"> 23,562 </td>
    <td class="tg-9wq8"> 52.5 / 16</td>
    <td class="tg-9wq8"> 2,492 </td>
    <td class="tg-9wq8"> 18,597 </td>
    <td class="tg-9wq8"> 539 </td>
    <td class="tg-9wq8"> 3,344 </td>
    <td class="tg-9wq8"> 331 </td>
    <td class="tg-9wq8"> 2,294 </td>
    <td class="tg-9wq8"> 449 </td>
    <td class="tg-9wq8"> 2,671 </td>

    <td class="tg-9wq8"> 4,125 </td>
    <td class="tg-9wq8"> 24,059 </td>
    <td class="tg-9wq8"> 52 / 15</td>
    <td class="tg-9wq8"> 3,022 </td>
    <td class="tg-9wq8"> 17,494 </td>
    <td class="tg-9wq8"> 678 </td>
    <td class="tg-9wq8"> 4,325 </td>
    <td class="tg-9wq8"> 369 </td>
    <td class="tg-9wq8"> 2,286 </td>
    <td class="tg-9wq8"> 734 </td>
    <td class="tg-9wq8"> 4,279 </td>
  </tr>
  <tr>
    <td class="tg-d459">Person</td>
    <td class="tg-9wq8"> 1,986 </td>
    <td class="tg-9wq8"> 8,555 </td>
    <td class="tg-9wq8"> 36 / 6</td>
    <td class="tg-9wq8"> 1,524 </td>
    <td class="tg-9wq8"> 7,076 </td>
    <td class="tg-9wq8"> 228 </td>
    <td class="tg-9wq8"> 945 </td>
    <td class="tg-9wq8"> 138 </td>
    <td class="tg-9wq8"> 523 </td>
    <td class="tg-9wq8"> 324 </td>
    <td class="tg-9wq8"> 956 </td>

    <td class="tg-9wq8"> 3,942 </td>
    <td class="tg-9wq8"> 13,236 </td>
    <td class="tg-9wq8"> 36 / 5</td>
    <td class="tg-9wq8"> 2,693 </td>
    <td class="tg-9wq8"> 9,629 </td>
    <td class="tg-9wq8"> 656 </td>
    <td class="tg-9wq8"> 2,297 </td>
    <td class="tg-9wq8"> 313 </td>
    <td class="tg-9wq8"> 1,155 </td>
    <td class="tg-9wq8"> 936 </td>
    <td class="tg-9wq8"> 2,452 </td>
  </tr>
  <tr>
    <td class="tg-lboi">MusicRecording</td>
    <td class="tg-9wq8"> 2,840 </td>
    <td class="tg-9wq8"> 7,606 </td>
    <td class="tg-9wq8"> 20 / 5 </td>
    <td class="tg-9wq8"> 2,278 </td>
    <td class="tg-9wq8"> 5,897 </td>
    <td class="tg-9wq8"> 566 </td>
    <td class="tg-9wq8"> 1,521 </td>
    <td class="tg-9wq8"> 248 </td>
    <td class="tg-9wq8"> 814 </td>
    <td class="tg-9wq8"> 314 </td>
    <td class="tg-9wq8"> 895 </td>

    <td class="tg-9wq8"> 508 </td>
    <td class="tg-9wq8"> 1,632 </td>
    <td class="tg-9wq8"> 26.5 / 5</td>
    <td class="tg-9wq8"> 323 </td>
    <td class="tg-9wq8"> 1,114 </td>
    <td class="tg-9wq8"> 77 </td>
    <td class="tg-9wq8"> 258 </td>
    <td class="tg-9wq8"> 42 </td>
    <td class="tg-9wq8"> 160 </td>
    <td class="tg-9wq8"> 143 </td>
    <td class="tg-9wq8"> 358 </td>
  </tr>
  <tr>
    <td class="tg-d459">Place</td>
    <td class="tg-9wq8"> 992 </td>
    <td class="tg-9wq8"> 5,647 </td>
    <td class="tg-9wq8"> 34.5 / 7</td>
    <td class="tg-9wq8"> 669 </td>
    <td class="tg-9wq8"> 4,498 </td>
    <td class="tg-9wq8"> 146 </td>
    <td class="tg-9wq8"> 940 </td>
    <td class="tg-9wq8"> 61 </td>
    <td class="tg-9wq8"> 428 </td>
    <td class="tg-9wq8"> 262 </td>
    <td class="tg-9wq8"> 721 </td>

    <td class="tg-9wq8"> 1,415 </td>
    <td class="tg-9wq8"> 7,420 </td>
    <td class="tg-9wq8"> 33 / 7</td>
    <td class="tg-9wq8"> 1,193 </td>
    <td class="tg-9wq8"> 6,518 </td>
    <td class="tg-9wq8"> 252 </td>
    <td class="tg-9wq8"> 1,425 </td>
    <td class="tg-9wq8"> 87 </td>
    <td class="tg-9wq8"> 466 </td>
    <td class="tg-9wq8"> 135 </td>
    <td class="tg-9wq8"> 436 </td>
  </tr>
  <tr>
    <td class="tg-lboi">LocalBusiness</td>
    <td class="tg-9wq8"> 5,056 </td>
    <td class="tg-9wq8"> 15,659 </td>
    <td class="tg-9wq8">45 / 9</td>
    <td class="tg-9wq8"> 3,848 </td>
    <td class="tg-9wq8"> 11,518 </td>
    <td class="tg-9wq8"> 1,077 </td>
    <td class="tg-9wq8"> 3,819 </td>
    <td class="tg-9wq8"> 538 </td>
    <td class="tg-9wq8"> 1,856 </td>
    <td class="tg-9wq8"> 670 </td>
    <td class="tg-9wq8"> 2,285 </td>

    <td class="tg-9wq8"> 5,883 </td>
    <td class="tg-9wq8"> 16,894 </td>
    <td class="tg-9wq8"> 46 / 9 </td>
    <td class="tg-9wq8"> 4,573 </td>
    <td class="tg-9wq8"> 12,372 </td>
    <td class="tg-9wq8"> 1,195 </td>
    <td class="tg-9wq8"> 3,699 </td>
    <td class="tg-9wq8"> 557 </td>
    <td class="tg-9wq8"> 2,026 </td>
    <td class="tg-9wq8"> 753 </td>
    <td class="tg-9wq8"> 2,496 </td>
  </tr>
  <tr>
    <td class="tg-d459">CreativeWork</td>
    <td class="tg-9wq8"> 1,223 </td>
    <td class="tg-9wq8"> 6,323 </td>
    <td class="tg-9wq8"> 26 / 6</td>
    <td class="tg-9wq8"> 601 </td>
    <td class="tg-9wq8"> 3,553 </td>
    <td class="tg-9wq8"> 237 </td>
    <td class="tg-9wq8"> 1,391 </td>
    <td class="tg-9wq8"> 269 </td>
    <td class="tg-9wq8"> 1,553 </td>
    <td class="tg-9wq8"> 353 </td>
    <td class="tg-9wq8"> 1,217 </td>

    <td class="tg-9wq8"> 2,073 </td>
    <td class="tg-9wq8"> 8,839 </td>
    <td class="tg-9wq8"> 33 / 6 </td>
    <td class="tg-9wq8"> 1,397 </td>
    <td class="tg-9wq8"> 6,437 </td>
    <td class="tg-9wq8"> 367 </td>
    <td class="tg-9wq8"> 1,671 </td>
    <td class="tg-9wq8"> 225 </td>
    <td class="tg-9wq8"> 1,063 </td>
    <td class="tg-9wq8"> 451 </td>
    <td class="tg-9wq8"> 1,339 </td>
  </tr>
  <tr>
    <td class="tg-lboi">MusicAlbum</td>
    <td class="tg-9wq8"> 937 </td>
    <td class="tg-9wq8"> 3,374 </td>
    <td class="tg-9wq8"> 18 / 5 </td>
    <td class="tg-9wq8"> 699 </td>
    <td class="tg-9wq8"> 2,369 </td>
    <td class="tg-9wq8"> 176 </td>
    <td class="tg-9wq8"> 660 </td>
    <td class="tg-9wq8"> 122 </td>
    <td class="tg-9wq8"> 511 </td>
    <td class="tg-9wq8"> 116 </td>
    <td class="tg-9wq8"> 494 </td>

    <td class="tg-9wq8"> 570 </td>
    <td class="tg-9wq8"> 1,484 </td>
    <td class="tg-9wq8"> 15 / 5 </td>
    <td class="tg-9wq8"> 397 </td>
    <td class="tg-9wq8"> 1,025 </td>
    <td class="tg-9wq8"> 98 </td>
    <td class="tg-9wq8"> 260 </td>
    <td class="tg-9wq8"> 53 </td>
    <td class="tg-9wq8"> 154 </td>
    <td class="tg-9wq8"> 120 </td>
    <td class="tg-9wq8"> 305 </td>
  </tr>
  <tr>
    <td class="tg-d459">Movie</td>
    <td class="tg-9wq8"> 1,377 </td>
    <td class="tg-9wq8"> 9,720 </td>
    <td class="tg-9wq8"> 85 / 8 </td>
    <td class="tg-9wq8"> 1,154 </td>
    <td class="tg-9wq8"> 8,850 </td>
    <td class="tg-9wq8"> 361 </td>
    <td class="tg-9wq8"> 2,983 </td>
    <td class="tg-9wq8"> 89 </td>
    <td class="tg-9wq8"> 435 </td>
    <td class="tg-9wq8"> 134 </td>
    <td class="tg-9wq8"> 435 </td>

    <td class="tg-9wq8"> 1,841 </td>
    <td class="tg-9wq8"> 10,631 </td>
    <td class="tg-9wq8"> 84 / 8 </td>
    <td class="tg-9wq8"> 1,351 </td>
    <td class="tg-9wq8"> 8,311 </td>
    <td class="tg-9wq8"> 353 </td>
    <td class="tg-9wq8"> 2,306 </td>
    <td class="tg-9wq8"> 180 </td>
    <td class="tg-9wq8"> 1,154 </td>
    <td class="tg-9wq8"> 310 </td>
    <td class="tg-9wq8"> 1,166 </td>
  </tr>
  <tr>
    <td class="tg-lboi">Hotel</td>
    <td class="tg-9wq8"> 1,491 </td>
    <td class="tg-9wq8"> 9,875 </td>
    <td class="tg-9wq8"> 36 / 9 </td>
    <td class="tg-9wq8"> 973 </td>
    <td class="tg-9wq8"> 6,701 </td>
    <td class="tg-9wq8"> 334 </td>
    <td class="tg-9wq8"> 2,299 </td>
    <td class="tg-9wq8"> 203 </td>
    <td class="tg-9wq8"> 1,412 </td>
    <td class="tg-9wq8"> 315 </td>
    <td class="tg-9wq8"> 1,762 </td>

    <td class="tg-9wq8"> 1,588 </td>
    <td class="tg-9wq8"> 9,618 </td>
    <td class="tg-9wq8"> 35 / 9</td>
    <td class="tg-9wq8"> 989 </td>
    <td class="tg-9wq8"> 6,175 </td>
    <td class="tg-9wq8"> 337 </td>
    <td class="tg-9wq8"> 2,120 </td>
    <td class="tg-9wq8"> 275 </td>
    <td class="tg-9wq8"> 1,812 </td>
    <td class="tg-9wq8"> 324 </td>
    <td class="tg-9wq8"> 1,631 </td>
  </tr>
  <tr>
    <td class="tg-d459">SportsEvent</td>
    <td class="tg-9wq8"> 1,407 </td>
    <td class="tg-9wq8"> 5,545 </td>
    <td class="tg-9wq8"> 43 / 4 </td>
    <td class="tg-9wq8"> 1,126 </td>
    <td class="tg-9wq8"> 4,659 </td>
    <td class="tg-9wq8"> 281 </td>
    <td class="tg-9wq8"> 965 </td>
    <td class="tg-9wq8"> 141 </td>
    <td class="tg-9wq8"> 434 </td>
    <td class="tg-9wq8"> 140 </td>
    <td class="tg-9wq8"> 452 </td>

    <td class="tg-9wq8"> 832 </td>
    <td class="tg-9wq8"> 3,292 </td>
    <td class="tg-9wq8"> 44 / 5</td>
    <td class="tg-9wq8"> 571 </td>
    <td class="tg-9wq8"> 2,527 </td>
    <td class="tg-9wq8"> 132 </td>
    <td class="tg-9wq8"> 584 </td>
    <td class="tg-9wq8"> 67 </td>
    <td class="tg-9wq8"> 289 </td>
    <td class="tg-9wq8"> 194 </td>
    <td class="tg-9wq8"> 476 </td>
  </tr>
  <tr>
    <td class="tg-lboi">Restaurant</td>
    <td class="tg-9wq8"> 1,508 </td>
    <td class="tg-9wq8"> 10,297 </td>
    <td class="tg-9wq8"> 43 / 17</td>
    <td class="tg-9wq8"> 1,182 </td>
    <td class="tg-9wq8"> 8,539 </td>
    <td class="tg-9wq8"> 215 </td>
    <td class="tg-9wq8"> 1,154 </td>
    <td class="tg-9wq8"> 111 </td>
    <td class="tg-9wq8"> 762 </td>
    <td class="tg-9wq8"> 215 </td>
    <td class="tg-9wq8"> 996 </td>

    <td class="tg-9wq8"> 1,176 </td>
    <td class="tg-9wq8"> 9,017 </td>
    <td class="tg-9wq8"> 42 / 17</td>
    <td class="tg-9wq8"> 948 </td>
    <td class="tg-9wq8"> 8,143 </td>
    <td class="tg-9wq8"> 124 </td>
    <td class="tg-9wq8"> 932 </td>
    <td class="tg-9wq8"> 33 </td>
    <td class="tg-9wq8"> 210 </td>
    <td class="tg-9wq8"> 195 </td>
    <td class="tg-9wq8"> 664 </td>
  </tr>
  <tr>
    <td class="tg-d459">Book</td>
    <td class="tg-9wq8"> 1,680 </td>
    <td class="tg-9wq8"> 9,323 </td>
    <td class="tg-9wq8"> 93.5 / 7 </td>
    <td class="tg-9wq8"> 1,123 </td>
    <td class="tg-9wq8"> 6,621 </td>
    <td class="tg-9wq8"> 247 </td>
    <td class="tg-9wq8"> 1,763 </td>
    <td class="tg-9wq8"> 204 </td>
    <td class="tg-9wq8"> 1,318 </td>
    <td class="tg-9wq8"> 353 </td>
    <td class="tg-9wq8"> 1,384 </td>

    <td class="tg-9wq8"> 2,540 </td>
    <td class="tg-9wq8"> 10,593 </td>
    <td class="tg-9wq8"> 120 / 7</td>
    <td class="tg-9wq8"> 1,717 </td>
    <td class="tg-9wq8"> 7,443 </td>
    <td class="tg-9wq8"> 559 </td>
    <td class="tg-9wq8"> 2,248 </td>
    <td class="tg-9wq8"> 346 </td>
    <td class="tg-9wq8"> 1,429 </td>
    <td class="tg-9wq8"> 477 </td>
    <td class="tg-9wq8"> 1,721 </td>
  </tr>
  <tr>
    <td class="tg-lboi">TVEpisode</td>
    <td class="tg-9wq8"> 464 </td>
    <td class="tg-9wq8"> 1,813 </td>
    <td class="tg-9wq8"> 64.5 / 5 </td>
    <td class="tg-9wq8"> 370 </td>
    <td class="tg-9wq8"> 1,594 </td>
    <td class="tg-9wq8"> 92 </td>
    <td class="tg-9wq8"> 301 </td>
    <td class="tg-9wq8"> 46 </td>
    <td class="tg-9wq8"> 109 </td>
    <td class="tg-9wq8"> 48 </td>
    <td class="tg-9wq8"> 110 </td>

    <td class="tg-9wq8"> 418 </td>
    <td class="tg-9wq8"> 1,194 </td>
    <td class="tg-9wq8"> 61 / 5</td>
    <td class="tg-9wq8"> 231 </td>
    <td class="tg-9wq8"> 829 </td>
    <td class="tg-9wq8"> 38 </td>
    <td class="tg-9wq8"> 114 </td>
    <td class="tg-9wq8"> 18 </td>
    <td class="tg-9wq8"> 56 </td>
    <td class="tg-9wq8"> 169 </td>
    <td class="tg-9wq8"> 309 </td>
  </tr>
  <tr>
    <td class="tg-d459">JobPosting</td>
    <td class="tg-9wq8"> 2,856 </td>
    <td class="tg-9wq8"> 11,227 </td>
    <td class="tg-9wq8"> 43 / 9 </td>
    <td class="tg-9wq8"> 2,132 </td>
    <td class="tg-9wq8"> 8,894 </td>
    <td class="tg-9wq8"> 509 </td>
    <td class="tg-9wq8"> 2,461 </td>
    <td class="tg-9wq8"> 240 </td>
    <td class="tg-9wq8"> 1,163 </td>
    <td class="tg-9wq8"> 484 </td>
    <td class="tg-9wq8"> 1,170 </td>

    <td class="tg-9wq8"> 147 </td>
    <td class="tg-9wq8"> 737 </td>
    <td class="tg-9wq8"> 42 / 8</td>
    <td class="tg-9wq8"> 110 </td>
    <td class="tg-9wq8"> 585 </td>
    <td class="tg-9wq8"> 30 </td>
    <td class="tg-9wq8"> 143 </td>
    <td class="tg-9wq8"> 12 </td>
    <td class="tg-9wq8"> 74 </td>
    <td class="tg-9wq8"> 25 </td>
    <td class="tg-9wq8"> 78 </td>
  </tr>
  <tr>
    <td class="tg-lboi">Museum</td>
    <td class="tg-9wq8"> 181 </td>
    <td class="tg-9wq8"> 647 </td>
    <td class="tg-9wq8"> 26 / 8 </td>
    <td class="tg-9wq8"> 145 </td>
    <td class="tg-9wq8"> 512 </td>
    <td class="tg-9wq8"> 41 </td>
    <td class="tg-9wq8"> 208 </td>
    <td class="tg-9wq8"> 18 </td>
    <td class="tg-9wq8"> 62 </td>
    <td class="tg-9wq8"> 18 </td>
    <td class="tg-9wq8"> 73 </td>

    <td class="tg-9wq8"> 53 </td>
    <td class="tg-9wq8"> 384 </td>
    <td class="tg-9wq8"> 23 / 9</td>
    <td class="tg-9wq8"> 49 </td>
    <td class="tg-9wq8"> 366 </td>
    <td class="tg-9wq8"> 5 </td>
    <td class="tg-9wq8"> 30 </td>
    <td class="tg-9wq8"> - </td>
    <td class="tg-9wq8"> - </td>
    <td class="tg-9wq8"> 4 </td>
    <td class="tg-9wq8"> 18 </td>
  </tr>
</tbody>
</table></div>

<p>
  The ground truth files are provided in the same format as datasets used in the SemTab challenge. The files include the <i>"table_name"</i> column, the <i>"column_index"</i> column and the <i>"label"</i> column. The first
  lists the names of the tables, the second points to the position of a column in the table and the last the annotation of the column at the specified index. Additionally, in the CPA ground truth files, the
  <i>"main_column_index"</i> points to the position in the table where the main column of the table is located. An example of ground truth files 
  is given in <i>Figure 2</i>.
</p>

<figure>
  <img src="images/ground_truth_files.png" alt="Ground Truth" width="85%">
  <figcaption><b><a id="Fig1"></a>Figure 2:</b> Example of ground truth files. The left table shows the CTA ground truth file and the right table shows the CPA ground truth file.</figcaption>
</figure>

<span id="toc6"></span>
<h2> 6. Testsets for Specific Challenges </h2>

<p>
  In addition to the full test sets for both tasks, we provide subsets of the test set that measure how good the systems under test can handle specific annotation challenges. 
  We provide test sets for the following challenges: (i) Missing Values, (ii) Value Format Heterogeneity and (iii) Corner Cases. The last test set is the Random subset. 
  In each subset and for each label included there is a minimum of 10 examples. These are explained in more detail below.    
</p>

<p>
  <b>(i) Missing values:</b> It is difficult for table annotation systems to correctly annotate columns in which many values are missing. In order to measure the annotation performance on such columns, 
  we provide a test set that contains columns that have a density between 10 and 70 percent, i.e. 10-70 percent of their rows are filled with values. For the CPA problem we included 3,572 column pairs 
  annotated with 125 CPA labels and for CTA 3,073 columns annotated with 64 CTA labels in this testset.
</p>

<p>
  <b>(ii) Value Format Heterogeneity:</b> As it can be difficult to distinguish columns whose values are expressed with different formats or metrics, we include this subset with columns that have the same label 
  but their values are expressed in different formats. The CPA columns that are selected for this
  challenge are the different date columns, duration, height, weight, width, price, telephone, faxNumber, servingSize, size and salary. There are in total 2,139 columns for CPA in this
  subset annotated with 30 different CPA labels.
  For CTA, the selected columns are duration, height, weight, width, Date, DateTime, Time, price, priceRange, telephone and faxNumber. The CTA columns are in total 957 and are annotated with
  10 CTA labels.
</p>

<p>
  <b>(iii) Corner Cases:</b> Corner Cases columns are columns that are considered difficult to annotate as they resemble columns with different labels or their values differ from other columns with the same label.
  This could lead to wrong predictions made by systems on these columns. Therefore we provide a subset that includes columns that are considered to be corner cases. There are 3,510 corner cases included in the
  CPA corner cases subset which are annotated with 106 different CPA labels, while in the CTA subset there are 5,142 corner case columns annotated with 70 different CTA labels.
</p>

<p>
  <b>(iv) Random Columns: </b> This subset includes randomly selected columns for each label of each task. In total there are 12,609 randomly selected columns annotated with 93 different labels
  for CTA and 15,663 randomly selected columns annotated with 180 different labels in the CPA subset.
</p>

<span id="toc7"></span>
<h2> 7. Baselines</h2>

<p>
  To evaluate the usefulness of the benchmark created sets, we ran two baseline experiments. The first baseline method is a non-deep learning method that uses TF-IDF weighting to create features 
  based on the column values
  and uses a Random Forest classifier to make predictions. The second baseline is a deep learning method called TURL [3] which uses a Transformer-based [4] architecture and is 
  pre-trained on relational tables using TinyBERT. In addition, TURL incorporates table context into its model. We fine-tune TURL for 50 epochs using a learning rate of 5e-5 and a batch size of 20.
  For the experiments the micro-F1 score is used to report the results of each baseline method and using both the Large training set and the Small training set. The results are summarized in <i>Table 4a</i> for CTA
  and in <i>Table 4b</i> for CPA.
</p>

<div class="tg-wrap"><table id="tg-X7nck" class="tg">
            <caption>Table 4a: Baseline Results for CTA</caption>

<thead>
  <tr>
    <th></th>
    <th class="tg-mkpc" colspan="2">Large Training Set</th>
    <th class="tg-ixdq" colspan="2">Small Training Set</th>
  </tr>
  <tr>
    <th class="tg-mkpc"></th>
    <th class="tg-ixdq">RF</th>
    <th class="tg-ixdq">TURL</th>
    <th class="tg-ixdq">RF</th>
    <th class="tg-ixdq">TURL</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-d459">Test (Full) </td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">59.48</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">76.89</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">54.30</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">69.84</span></td>
  </tr>
  <tr>
    <td class="tg-lboi"> Test (Missing Values) </td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">65.31</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">76.86</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">61.70</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">70.09</span></td>
  </tr>
  <tr>
    <td class="tg-d459"> Test (Format Heterogeneity) </td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">64.68</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">88.82</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">59.56</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">82.86</span></td>
  </tr>
  <tr>
    <td class="tg-lboi">Test (Corner Cases)</td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">57.29</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">71.57</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">53.66</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">65.07</span></td>
  </tr>
  <tr>
    <td class="tg-lboi">Test (Random Columns)</td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">58.56</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">78.16</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">52.38</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">70.74</span></td>
  </tr>
</tbody>
</table>
					
<table id="tg-FFQ1F" class="tg">
  <caption>Table 4b: Baseline Results for CPA</caption>
  <thead>
  <tr>
    <th></th>
    <th class="tg-mkpc" colspan="2">Large Training Set</th>
    <th class="tg-ixdq" colspan="2">Small Training Set</th>
  </tr>
  <tr>
    <th class="tg-mkpc"></th>
    <th class="tg-ixdq">RF</th>
    <th class="tg-ixdq">TURL</th>
    <th class="tg-ixdq">RF</th>
    <th class="tg-ixdq">TURL</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-d459">Test (Full) </td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">48.22</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">68.88</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">42.72</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">60.68</span></td>
  </tr>
  <tr>
    <td class="tg-lboi"> Test (Missing Values) </td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">46.58</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">63.86</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">41.29</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">56.49</span></td>
  </tr>
  <tr>
    <td class="tg-d459"> Test (Format Heterogeneity) </td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">58.44</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">78.40</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">53.25</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">68.91</span></td>
  </tr>
  <tr>
    <td class="tg-lboi">Test (Corner Cases)</td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">45.33</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">63.39</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">39.77</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">56.13</span></td>
  </tr>
  <tr>
    <td class="tg-lboi">Test (Random Columns)</td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">47.85</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">69.96</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">42.27</span></td>
    <td class="tg-kyy7"><span style="font-weight:400;font-style:normal">61.53</span></td>
  </tr>
</tbody>
</table></div>

<p>
  Test (Full) refers to the full test set that includes all annotated columns, while Test (Missing Values) refers to the subset of the full
  test set that contains the columns that have missing values, Test (Format Heterogeneity) refers to the subset of the full test set that contains
  the columns with different value formats , Test (Corner Cases) refers to the subset of the full test set that contains the hard to annotate
  columns and lastly Test (Random Columns) contains all the columns of the full test set that were chosen randomly. 
</p>

<p>
  In all cases, TURL achieves higher micro-F1 scores when trainined on the Large and Small training sets. The difference between both baselines
  is on average 17 percetange points on all test sets. The Test (Corner Cases) and Test (Missing Values) are two of the subsets where the lowest
  micro-F1 scores are reached for both baselines, while when evaluating on Test (Format Heterogeneity) both methods appear to achieve better performance
  than in other subsets. Additionally, training with less examples drops the F1 scores and this can be seen more in TURL. A more fine-grained evaluation
  and their results for each label in each task and each challenge can be found in the <a href="#toc8">Download</a> section. 
  We list in <i>Table 5</i> the top 5 labels that reached the highest score in the overall test set and the top 5 labels that reached the lowest score for both CPA and CTA tasks in the overall test set.
 </p>

 <div class="tg-wrap"><table id="tg-X7nck" class="tg">
  <caption>Table 5a. Top 5 best and 5 lowest F1 scores for labels in CTA for TURL using the Large training set</caption>

<thead>
  <tr>
      <th class="tg-mkpc" colspan="2">Top 5</th>
      <th class="tg-mkpc" colspan="2">Lowest 5</th>
    </tr>
  <tr>
      <td>Label</td>
      <td>Overall F1</td>
      <td>Label</td>
      <td>Overall F1</td>
    </tr>
</thead>
<tbody>
  <tr>
    <td>Action</td>
    <td>97.62</td>
    <td>identifierNameAP</td>
    <td>27.00</td>
  </tr>
  <tr>
    <td>EventAttendanceModeEnumeration</td>
    <td>97.33</td>
    <td>MusicGroup</td>
    <td>25.00</td>
  </tr>
  <tr>
    <td>email</td>
    <td>96.84</td>
    <td>JobPosting/name</td>
    <td>24.06</td>
  </tr>
  <tr>
    <td>EventStatusType</td>
    <td>96.45</td>
    <td>CreativeWork/name</td>
    <td>21.40</td>
  </tr>
  <tr>
    <td>ItemAvailability</td>
    <td>95.88</td>
    <td>CreativeWorkSeries</td>
    <td>15.38</td>
  </tr>
</tbody>
</table>
					
<table id="tg-FFQ1F" class="tg">
  <caption>Table 5b. Top 5 best and 5 lowest F1 scores for labels in CPA for TURL using the Large training set</caption>
  <thead>
  <tr>
      <th class="tg-mkpc" colspan="2">Top 5</th>
      <th class="tg-mkpc" colspan="2">Lowest 5</th>
    </tr>
  <tr>
      <td>Label</td>
      <td>Overall F1</td>
      <td>Label</td>
      <td>Overall F1</td>
    </tr>
</thead>
<tbody>
  <tr>
    <td>knowsLanguage</td>
    <td>99.50</td>
    <td>awayTeam</td>
    <td>16.99</td>
  </tr>
  <tr>
    <td>contactType</td>
    <td>99.09</td>
    <td>parentOrganization</td>
    <td>16.00</td>
  </tr>
  <tr>
    <td>interactionType</td>
    <td>98.68</td>
    <td>award</td>
    <td>15.38</td>
  </tr>
  <tr>
    <td >makesOffer</td>
    <td>98.17</td>
    <td>alternateName</td>
    <td>15.38</td>
  </tr>
  <tr>
    <td>dayOfWeek</td>
    <td >97.10</td>
    <td >homeTeam</td>
    <td>8.96</td>
  </tr>
</tbody>
</table></div>

<span id="toc8"></span>
<h2> 8. Download</h2>
			
        <p>We offer the WDC SOTAB benchmark for public download. In the table below you will find download links for training, validation and testing splits for both tasks, CPA and CTA. Each zip file contains a folder with the tables
		as well as a csv file containing the label annotations for each table. This file consists of three columns, (i) table name, (ii) column position in the table and (iii) associated CPA/CTA label for this column.
		
		For both types of files we also offer a small sample file as an example and quick reference on their structure.</p>
		
		The tables are represented using the JSON format and can for example be easily processed using the <a href="https://pandas.pydata.org/">pandas</a> Python library.<br><br>
			<code>import pandas as pd<br>
						df = pd.read_json('table_name.json.gz', compression='gzip', lines=True)
				</code>
				<p>
		</p>
			<p>
<table>
					<tbody>
						<tr>
							<td><b>Task</b></td>
							<td><b>Training Set</b></td>
							<td><b>Size</b></td>
							<td><b>Validation Set</b></td>
							<td><b>Size</b></td>
							<td><b>Test Set</b></td>
							<td><b>Size</b></td>
							<td><b>Table file sample</b></td>
							<td><b>Label file sample</b></td>
						</tr>



						<tr>
							<td>Column Property Annotation (CPA)</td>
							<td><a
									href="http://data.dws.informatik.uni-mannheim.de/structureddata/sotab/CPA_Training.zip">CPA_Training.zip</a>
								</td>
							<td>1.2GB</td>
							<td><a
									href="http://data.dws.informatik.uni-mannheim.de/structureddata/sotab/CPA_Validation.zip">CPA_Validation.zip</a>
							</td>
							<td>200MB</td>
							<td><a
									href="http://data.dws.informatik.uni-mannheim.de/structureddata/sotab/CPA_Test.zip">CPA_Test.zip</a>
								</td>
							<td>335MB</td>
							<td><a
									href="#">CPA_sample_table</a>
							</td>
							<td><a
									href="#">CPA_sample_labels</a>
							</td>
						</tr>
						<tr>
    <td>Column Type Annotation (CTA)</td>
    <td><a
            href="http://data.dws.informatik.uni-mannheim.de/structureddata/sotab/CTA_Training.zip">CTA_Training.zip</a>
        </td>
	<td>1.2GB</td>
    <td><a
            href="http://data.dws.informatik.uni-mannheim.de/structureddata/sotab/CTA_Validation.zip">CTA_Validation.zip</a>
    </td>
	<td>170MB</td>
    <td><a
            href="http://data.dws.informatik.uni-mannheim.de/structureddata/sotab/CTA_Test.zip">CTA_Test.zip</a>
        </td>
	<td>300MB</td>
    <td><a
            href="#">CTA_sample_table</a>
    </td>
    <td><a
            href="#">CTA_sample_labels</a>
    </td>
</tr>
					</tbody>
				</table>

  <h3>Additional Downloads</h3>
  <p>
    <table>
      <tbody>
        <tr>
          <td></td>
          <td>File</td>
          <td>Size</td>
        </tr>
        <tr>
          <td><b>Detailed Baseline Results</b></td>
          <td>
            <a href="http://data.dws.informatik.uni-mannheim.de/structureddata/sotab/Baseline Results.xlsx">Baseline Results.xlsx</a>
          </td>
          <td>149 KB</td>
        </tr>
        <tr>
          <td><b>CTA and CPA Labels</b></td>
          <td>
            <a href="http://data.dws.informatik.uni-mannheim.de/structureddata/sotab/CPA and CTA Labels.xlsx">CPA and CTA Labels.xlsx</a>
          </td>
          <td>25 KB</td>
        </tr>
      </tbody>  
    </table>
  </p>
				<span id="toc9"></span>
				<h2>9. Other Table Annotation Benchmarks</h2>

        <p>
          There exist several other benchmark datasets for the CTA and CPA task. An overview of current results on these datasets is given at Papers with Code for Column Type Annotation, Column Property Annotation and Cell Entity Annotation.
          The two benchmarks that are most closely related to SOTAB are GitTables-SemTab [5] and Wikitables-TURL [3]. 
        </p>

        <p>
          <a href="https://gittables.github.io/">GitTables</a> [5] is a corpus that contains around 1 million tables collected from GitHub and annotated with Schema.org and 
          DBpedia classes and properties. A <a href="https://zenodo.org/record/5706316#.YvuQdnZByUk">subset of GitTables</a> was created for the SemTab challenge, where 1,101 tables were annotated 
          and made available for benchmarking methods on the CTA task. In these tables, 721 columns were annotated with 59 unique Schema.org types and properties 
          and 2,533 columns were annotated using 122 unique DBpedia classes.
        </p>

        <p>
          The <a href="https://github.com/sunlab-osu/TURL">WikiTables-TURL</a>[3] datasets were created to evaluate methods on the CTA and CPA tasks. It contains 406,706 tables annotated for 
          the CTA task using 255 unique labels from Freebase and 55,970 tables annotated for CPA using 121 unique labels. They provide fixed 
          training, validation and test sets. Over 600,000 columns are annotated for CTA and over 60,000 column pairs for CPA. 
        </p>
				
				<span id="toc10"></span>
				<h2> 10. References </h2>

        <p>
          [1] D. Ritze, O. Lehmberg, Y. Oulabi, C. Bizer, Profiling the Potential of Web Tables for
          Augmenting Cross-domain Knowledge Bases, in: Proceedings of the 25th International
          Conference on World Wide Web, 2016, pp. 251–261. <br/>

          [2] E. Rahm, P. A. Bernstein, A survey of approaches to automatic schema matching, The
          VLDB Journal — The International Journal on Very Large Data Bases 10 (2001) 334–350. <br/>

          [3] X. Deng, H. Sun, A. Lees, Y. Wu, C. Yu, TURL: Table understanding through representation
          learning, Proceedings of the VLDB Endowment 14 (2020) 307–319. <br/>

          [4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, et al., Attention is All you Need,
          in: Advances in Neural Information Processing Systems (NIPS 2017), volume 30, 2017. <br/>

          [5] M. Hulsebos, Ç. Demiralp, P. Groth, GitTables: A Large-Scale Corpus of Relational Tables,
          arXiv:2106.07258 (2022). <br/>
        </p>
				

        <p>Please send questions and feedback to the <a
						href="http://groups.google.com/group/web-data-commons">Web Data
						Commons Google Group</a>.<br /><br />
					More information about the <strong>Web Data Commons</strong> project is found <a href="http://webdatacommons.org/">here</a>.</p>
				
				<p>&nbsp;</p>

				<script type="text/javascript">
					$('#toc').toc({
						'selectors': 'h2', //elements to use as headings
						'container': '#toccontent', //element to find all selectors in
						'smoothScrolling': true, //enable or disable smooth scrolling on click
						'prefix': 'toc', //prefix for anchor tags and class names
						'highlightOnScroll': true, //add class to heading that is currently in focus
						'highlightOffset': 100, //offset to trigger the next headline
						'anchorName': function (i, heading, prefix) { //custom function for anchor name
							return prefix + i;
						}
					});
					$('[id*="link_"]').each(function () {
						var element = $(this);
						element.click(function (e) {
							e.preventDefault();
							var id = element.attr("id").split("_")[1];
							element.parent().removeClass("show").addClass("no-show");
							$('#charts_' + id).removeClass("no-show").addClass("show");
						});
					});
					$('[id*="colapse_"]').each(function () {
						var element = $(this);
						element.click(function (e) {
							e.preventDefault();
							var id = element.attr("id").split("_")[1];
							element.parent().removeClass("show").addClass("no-show");
							$('#intro_' + id).removeClass("no-show").addClass("show");
						});
					});
					document.getElementById("defaultOpen").click();
				</script>
</body>

</html>
